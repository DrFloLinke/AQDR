[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": " Analysing Quantitative Data with R",
    "section": "",
    "text": "Preface\nWelcome to Analysing Quantitative Data with R! The purpose of this module is to introduce you to key components of the statistical programming language R and to demonstrate some of the most commonly used methods than can be applied to preprocess and analyse quantitative data.\nThe module is divided into four parts. Please work through each part in your own time, and ensure to complete the exercises at the end of each chapter.\nIf you have any queries, please feel free to get in touch.\n\n \n\n\n\nAccessibility\nThe companion uses the font “Lexend”. Lexend fonts are intended to reduce visual stress and so improve reading performance. Initially they were designed with dyslexia and struggling readers in mind, but Bonnie Shaver-Troup, creator of the Lexend project, soon found out that these fonts are also great for everyone else.\nCode is displayed in Recursive Mono. The font’s characters share the same width for clear legibility and perfect alignment. This is particularly helpful for use in programming and data-heavy design tasks, but also allows for creative possibilities in display typography.\nEquations and mathematical expressions are set in Fira Math. Fira Math is easy to read with open shapes, clear spacing, and good contrast. This keeps subscripts, superscripts, and fractions legible - even at small sizes. It stays readable on screens and in print, reducing mix-ups between look-alike characters like 1, I, and l.\nThe companion also uses a dark mode theme. For many users, including some neurodivergent individuals, dark mode can reduce eye strain and enhance focus by minimising visual overstimulation.\nWhilst I recognise that mobile phones play an essential part in all our daily lives, please note that this companion is optimised for display on tablets, laptops, and desktop computers.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "01-introduction.html",
    "href": "01-introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis section provides a basic overview of the R programming language and instructions on how to set up R and RStudio on your computer.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#what-is-r-and-why-should-i-learn-it",
    "href": "01-introduction.html#what-is-r-and-why-should-i-learn-it",
    "title": "Introduction",
    "section": "What is R and why should I learn it?",
    "text": "What is R and why should I learn it?\nR is an open-source statistical programming language used for data processing, analysis and visualization. It was released in 1993 by University of Auckland and is based on the older S programming language. Since then, it has grown to be one of the most popular languages for quantitative data analysis across multiple disciplines.\nThere are many reasons why learning R is worth your time:\n1. It’s open-source - as opposed to other popular statistical packages such as SPSS, Stata or SAS, R is an open-source tool. This means that using R and all its public libraries is completely free of charge and requires no special licences. It is maintained by the user community, and thus all the improvements directly coincide with the requirements of the end-users, rather than profit-driven considerations often guiding the introduction of new functionalities to proprietary statistical packages.\n2. It can be widely applied - since it’s developed by users, there are tools in R for doing almost everything, from simple data manipulation and visualization to automated web data collection, natural language processing, survey data analysis, epidemiology, computational biology, social network analysis, cognitive modelling, geospatial analysis, deep learning and many more.\n\n3. It is widely applied - in 2019, R was the 5th most popular tool for data science mentioned by job advertisements on indeed.com. It is widely used in a variety of industries, including tech, consulting, think-tanks and public institutions.\n4. It has an abundance of resources available on-line - due to its increase in popularity in recent years, there are multiple R courses available on-line, which help you master the usage of particular techniques and libraries through practical exercises. The considerable size of the community that emerged surrounding the language throughout the recent decades means that almost all of the problems you will face on different levels of advancement in R programming are likely to have been encountered by others in the past. As a result, Google is a tremendous resource for troubleshooting, debugging and deepening your understanding of the ins and outs of R programming. Examples of useful MOOC course websites include DataCamp, Coursera and Edex.\n5. It is a programming language for analysing data. While this may sound like a cliché, the ubiquity of data is transforming almost every area of life nowadays. Therefore data literacy and understanding of key programming concepts are extremely useful in itself, as you never know when you may need these in the future. Majority of the concepts covered in this course are to a large extent transferable to other commonly used languages and data analysis tools. Having a high level of technical and data analysis skills is one of the most desirable skills according to employers - a fact which is largely overlooked by many prospective job seekers:",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#console-pane",
    "href": "01-introduction.html#console-pane",
    "title": "Introduction",
    "section": "Console pane",
    "text": "Console pane\n\nThe console is where all the output generated by R (except for plots) goes. You can also enter R Studio function calls in there to produce output. Give it a try and type print(\"Hello world!\") into the command line.\n\nprint(\"Hello world!\")\n\nYou can also try entering simple mathematical calculations such as 2+2 and see that R works perfectly as a calculator. Note that you should rarely enter longer pieces of code into the console. To ensure reproducibility of your work, the code should always be executed from saved R Scripts.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#source-pane",
    "href": "01-introduction.html#source-pane",
    "title": "Introduction",
    "section": "Source pane",
    "text": "Source pane\n\nThe source pane contains the R Scripts - these are essentially text files in which you save longer pieces of R code that you can execute sequentially. You can open a new R-Script by clicking on the icon in the top left corner or by pressing Ctrl + Shift + N in Windows (or Command + Shift + N in Macbooks). You can then save the R script by clicking the floppy disk icon at the top of your R script or by pressing Ctrl + S (Command + S). Please save your file and name it hello.R. Make sure to create a separate folder such as r_course in which you will store all the files related to the lessons. You can then type the same print(\"Hello world!\") function call as you did into the console and execute it by selecting the line and pressing Ctrl + Enter (Command + Enter) or pressing the small icon at the top right corner. Generally, you can execute longer pieces of code by first selecting the entire chunk using your cursor and then pressing the appropriate execution keyboard shortcut or clicking on the execution button. You can also comment on your code by using the # hashtag. Everything following # will not be interpreted by R as code, therefore you can write anything you want after it until you begin a new line by pressing Enter. Generally, it’s crucial to comment your code, as when you come back to some of your scripts in half a year, you are likely to not remember why and how you did everything. So rather than keeping print(\"Hello world!\") in your first R script, type:\n\nprint(\"Hello world!\") #this function prints it input to the console\n\n\nA short note on reproducibility\nIt is vital that all the essential code that you work on both during this course and in all future cases of R usage is stored in R scripts rather than executed directly from the console. An ideal R project would consist of the file containing the data you have used in the format it was collected (in case of primary data) or provided (in case of secondary data) and an R script(s) used for the analysis with appropriate comments made at each step, including data preprocessing, visualization and any statistical analyses you run as part of the project. This way, anyone who wants to reproduce your analysis (including yourself in a year) can simply open the script, “press play” and get the same results again, as well as trace back all the steps and decisions you made to arrive at these results. This is crucial in academia, as it allows your supervisor/reviewer to ensure methodological correctness of your work, which is an important step in tackling the replication crisis haunting many academic disciplines. It is also very important in a business setting, as it allows your co-workers to understand your work, find possible errors in it or re-use it when necessary, rather than looking at it as a black box producing an output of potentially questionable quality.\nEssential code here refers to anything needed to reproduce your analysis. Some examples of non-essential operations you may still execute from your console include getting an overview of the data (for example printing the variable names, number of records, variable types or producing a simple plot that you are not going to include in your final work) as well as looking at function documentation described below. However, for this course, it’s recommended that you keep all the code you use in your R scripts. This will allow you to come back to each lesson and review all the tools that you have learned throughout the course. Every section will provide you with an R script with all the code that it uses in order at the top of the page. It will also include the R markdown file (.Rmd) with the same content. Rmarkdown is a special format used to produce documents in formats such as HTML, pdf or Docx mixing text and R code - you will learn about it in the R Markdown section at the end of this course.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#environment-pane",
    "href": "01-introduction.html#environment-pane",
    "title": "Introduction",
    "section": "Environment pane",
    "text": "Environment pane\nThe third tab provides a list of R Environments. While an important idea in R, they are irrelevant for this course. For more advanced readers, Hadley Wickham’s Advanced R provides a good overview of the concept. During this course, you will only work with R’s Global Environment. This is where all the objects you assign in R will be stored and displayed. For example, before you have printed the output of 2 + 2 to the R console. However, you can also store the same output in an object of (almost) arbitrary name, say x, using the &lt;- assignment operator. The output that would normally be stored in the console will instead be passed into the object on its left. You can then type x in your script and execute it or type x into the console and press enter to see the value stored in x, as seen below.\n\nx &lt;- 2 + 2\nx\n\n[1] 4\n\n\nNote that x appeared in the environment pane under the Values section. Similarly, we can create a function, by typing:\n\nfoo &lt;- function() 2 + 2\n\n\nAgain, function foo will appear in your environment with an appropriate label. You will learn more about assignment and functions in R Programming Basics and Key Programming Concepts. Here, the most important takeout point is that the “Environment” tab allows you to see all the objects you have defined in R, along with some of their key properties.\n\nA short note on keeping your working environment clean\nWhile working on an R project, it’s important to always start with a clean environment. Especially when you start receiving an error that you cannot trace back easily, the best first step is often to clean your run session and run your script run by line carefully examining the output. To clean the working environment, you should simply restart your R Studio session, either by selecting the “Restart R” option from the “Session” tab at the top of the window or by pressing Ctrl + Shift + F10 (Command + Shift + F10). Note that this will cause you to lose everything that is stored in your environment. This is yet another reason to put all your code in a script - as a result, even when you restart the session, you should be able to reproduce everything that was previously present in your Global Environment.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-introduction.html#filesplothelp-pane",
    "href": "01-introduction.html#filesplothelp-pane",
    "title": "Introduction",
    "section": "Files/plot/help … pane",
    "text": "Files/plot/help … pane\nThe last pane contains a file browser, a plot viewer and documentation. You will learn more about plotting data in the chapters on Exploratory analysis and Data visualization. More importantly, this section is also where you can view the help coming from R documentation. Every function in R has an offline documentation file associated with it that you can access while using R Studio. To do it simply type help(functionname) or a question mark ? followed by the function name. Give it a try by typing ?sum into the R console and pressing enter - you should be able to see the help for the sum function. The documentation is extremely useful when working with R. While its content may seem a bit technical at first, as you learn more about R, a lot of things should become clearer, making it easier to learn and understand new functionalities. Furthermore, it usually provides reproducible examples of R function’s usage (you need to scroll to the very bottom of the documentation to find it), which allow you to understand in what context and how can the function be used. For example, in the sum function documentation, we can see:\n\n## Pass several numbers to sum, and it also adds the elements.\nsum(1, 2, 3, 4, 5)\n\nGive it a try in your hello.R R script - you should see the sum function behaving exactly as expected.\n\nA short note on finding help when working with R\n\nWhile the built-in documentation is useful, sometimes you may run into trouble that’s difficult to work out on your own. The simplest solution is to concisely describe your problem (or copy the error message you are receiving) and Google it. Usually, the first result that will show will come from StackOverflow. StackOverflow is a Q & A website focused on programming in all languages, and has an extensive section solely on R (almost 350 000 questions at the time of this course creation). For example, you can try typing \"a\" + 5 into the console. The resulting error message Error in \"a\" + 5 : non-numeric argument to binary operator is rather complicated, and might put many first-time users off. However, a quick Google search will lead you to this Stack Overflow post, which explains that A binary operation is a calculation that takes two values (operands) and produces another value (…) When you see that error message, it means that you are (or the function you’re calling is) trying to perform a binary operation with something that isn’t a number. Note that it’s good to make sure that your question has not been answered previously (which is the case 99 out of 100 times) before posting a new one. Should you decide to post your question, make sure to follow Stack Overflow guidelines, as well as to make your example reproducible, as described in this post.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-basics.html",
    "href": "02-basics.html",
    "title": "Basics of R Programming",
    "section": "",
    "text": "Arithmetic operations and assignment\nR allows you to perform every basic mathematical operation, so it can simply be used as a calculator. Below, you can see several examples.\n5 + 3 #addition\n5 - 3 #subtraction\n5 / 3 #division\n5 / 0\n5 * 3 #multiplication\n5 ^ 3 #exponentiation\nsqrt(4) #square root\n(5 + 5) / (3 + 1) #parenthesis\n9 %% 2 #modulo\n9 %/% 2 #integer division\nlog(2) #natural logarithim\nexp(2) #exponent\nAll the operations performed above generate some output, which is printed to the R console. Most of the time, however, it is useful to store the outputs of the operations we perform, so that they can be accessed repeatedly. To do that, we can store values in variables, which can be viewed and manipulated by referencing their names. Values are assigned to variables using the &lt;-operator. While = may seem equivalent, for more advanced reasons &lt;- is used whenever assigning a value. If you’re interested in the technical details behind it, you can check out this Stack Overflow post.\nx &lt;- 5\nmy_number &lt;- 4.2\nresult &lt;- 5 + 6 * 7 - 8\nYou can name a variable anything you like, however it cannot:\nIt is also useful to avoid naming variables using names that are already defined in R to be something else - however, you will learn the avoid this as you progress in the course.\nKeep in mind, that while variable names are arbitrary, it’s good to keep them concise and informative, especially if you have to present your code to someone or to come back to your own work after a long period of time.\nAssigning the values does not produce any output. To access the values assigned to a variable, you have to call its name directly in the script or the console. You can use this to perform mathematical operations on the objects as well.\nx &lt;- 5\ny &lt;- 7\nx\ny\nx + y\nz &lt;- x * y\nz\nFinally, you can always assign a new value to the same name. However this implies that the old value will be discarded. It can be useful when you know that you won’t need to access the value again. For example, it’s a common practice to modify a variable already defined such that x &lt;- x + 5. This simply means “add 5 to x and store it in x”.\nx &lt;- 5\nx\n\n[1] 5\n\nx &lt;- 7\nx\n\n[1] 7\n\nx &lt;- x + 2",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basics of R Programming</span>"
    ]
  },
  {
    "objectID": "02-basics.html#coercion",
    "href": "02-basics.html#coercion",
    "title": "Basics of R Programming",
    "section": "Coercion",
    "text": "Coercion\nAnother important topic when dealing with vectors is coercion. This refers to forcing one vector type to become another using the as functions. For example, we use as.character to force an R object into a character vector, or as.numeric to force it into a numeric vector:\n\nnumbers_char &lt;- c(\"5\",\"6\",\"7\")\nnumbers_char\n\n[1] \"5\" \"6\" \"7\"\n\nnumbers &lt;- as.numeric(numbers_char)\nnumbers\n\n[1] 5 6 7\n\n\n\nnumbers &lt;- c(10, 123, 12)\nas.character(numbers)\n\n[1] \"10\"  \"123\" \"12\" \n\n\nNote that this will not always work, as in many cases elements of one vector type cannot be interpreted as another. For example:\n\nnums &lt;- c(\"1\",\"2\",\"three\")\nas.numeric(nums)\n\n[1]  1  2 NA\n\n\nIn this case, R still returns the output, however the third element of the nums vector is turned into an NA value. NA is shorthand for Not Available - it’s a constant that R uses to deal with missing values. This is indicated by the warning printed by R to the console. Missing values will be covered in more detail in the next chapter.\n\nLogical Values and Operators\nAnother crucial type of operations in R are logical operations, also known as boolean. They are used to evaluate the truth value of logical statements such as variable “A is equal to variable B” or variable A is a numeric vector. Whenever the queried statement is True, they return TRUE and FALSE otherwise. Below you can see some simple examples using the equality operator == - the double equality means that we are checking whether two values are equal, rather than assigning one to another.\n\na &lt;- 5\na == 5\n\n[1] TRUE\n\na == 3\n\n[1] FALSE\n\na - 2 == 3\n\n[1] TRUE\n\n\"John\" == \"James\"\n\n[1] FALSE\n\n\nThe ! operator is used for negation, so !TRUE results in FALSE and vice versa. Accordingly, != is used to denote ‘not equals to’.\n\n!TRUE\n\n[1] FALSE\n\n!FALSE\n\n[1] TRUE\n\n5 != 6\n\n[1] TRUE\n\n\"John\" != \"James\"\n\n[1] TRUE\n\n\nLogical operations can also be used to compare values, by using a &lt; b for “a is less than b”, a &lt;= b for “a is less or equal to b” and vice versa.\n\nx &lt;- 5\ny &lt;- 3\n\nx &gt; y\n\n[1] TRUE\n\nx - 2 &lt; y\n\n[1] FALSE\n\nx - 2 &lt;= y\n\n[1] TRUE\n\n\nFinally, the & (logical “and”) and | (logical “or”) operators are designed to combine TRUE/FALSE values. So, if you put & between two logical values, it will yield TRUE if and only if both values are TRUE. | on the other hand will return TRUE if any of the values is TRUE\n\nTRUE & TRUE\n\n[1] TRUE\n\nTRUE & FALSE\n\n[1] FALSE\n\nTRUE | FALSE\n\n[1] TRUE\n\nFALSE | FALSE\n\n[1] FALSE\n\n(5 + 1 == 6) | (2 + 2 == 5)\n\n[1] TRUE\n\n(5 + 1 == 6) & (2 * 2 == 10)\n\n[1] FALSE\n\n\nThe logical values are also often used to verify whether we are dealing with a certain R type - for example to check whether a value is a character or numeric. This is achieved by using the is functions, such as is.numeric or is.character.\n\nnumbers &lt;- c(5, 6, 7)\nis.vector(numbers)\n\n[1] TRUE\n\nis.numeric(numbers)\n\n[1] TRUE\n\nis.character(numbers)\n\n[1] FALSE\n\nwords &lt;- c(\"Word\",\"Word\")\nis.numeric(words)\n\n[1] FALSE\n\nis.character(words)\n\n[1] TRUE\n\n\nAs with numbers and characters, the logical values also form their special types of vectors and can be used to perform element-wise operations.\n\na &lt;- c(TRUE, FALSE, FALSE)\nb &lt;- c(TRUE, TRUE, TRUE)\na & b\n\n[1]  TRUE FALSE FALSE\n\n\nThey can also be used to find whether each value in a numeric or character vector is equal to another.\n\nx &lt;- c(5, 6, 7, 8)\nx == 5\n\n[1]  TRUE FALSE FALSE FALSE\n\ny &lt;- c(\"John\", \"James\", \"Thomas\")\nz &lt;- c(\"John\",\"James\",\"Robert\")\nz == y\n\n[1]  TRUE  TRUE FALSE\n\n\nThe boolean vectors can be also thought of as a special case of numeric vectors consisting only of 0s and 1s, where 0 corresponds with FALSE and 1 with TRUE value. This can be easily seen in the example below:\n\nTRUE + TRUE + TRUE\n\n[1] 3",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basics of R Programming</span>"
    ]
  },
  {
    "objectID": "02-basics.html#indexing",
    "href": "02-basics.html#indexing",
    "title": "Basics of R Programming",
    "section": "Indexing",
    "text": "Indexing\nWhile a very large volume of data can be stored in one vector, we often may want to access only a specific element of it, or a fraction of the elements. An index of a vector, is simply an integer corresponding to the position of a value in a the vector. So, a vector with N values has integers ranging from 1 to N. For example, in vector c(5, 10, 3, 2), the index of 5 is 1, the index of 10 is 2, the index of 3 is 3, etc. Indexing is an operation of accessing a vector’s elemet at a given index, using the square brackets []. For example, a[5] means “get the fifth element from vector a”.\n\na &lt;- c(5.2, 4.5, 6.2, 8.9, 10.2, 4.8, 0.1)\na[5]\n\n[1] 10.2\n\n\n\nIndexing can also be used to replace values at a given position in a vector. In the example below, we replace the first element of a with the number 1000.\n\na[1] &lt;- 1000\na \n\n[1] 1000.0    4.5    6.2    8.9   10.2    4.8    0.1\n\n\nIndexing can also be done using another vector of numeric values. For example we may want to get the first, second and fifth elements of a given vector, or a sequence of elements between 1 and 4.\n\na &lt;- c(5.2, 4.5, 6.2, 8.9, 10.2, 4.8, 0.1)\na[c(1, 3, 5)]\n\n[1]  5.2  6.2 10.2\n\n#equivalent to:\nb &lt;- c(1, 3, 5)\na[b]\n\n[1]  5.2  6.2 10.2\n\n#can also be done for a sequence\na[1:5]\n\n[1]  5.2  4.5  6.2  8.9 10.2\n\n\n\nIndexing is even more powerful in conjunction with logical operations. This is because, a logical vector can be used to index any vector - such indexing operations returns all the values of the indexed vector where the corresponding indexing logical vector is TRUE. This may sound confusing at first, but is actually quite straightforward, as seen below:\n\nx &lt;- c(4.2, 5.6, 7.2, 1.1)\nindex &lt;- c(FALSE, TRUE, TRUE, FALSE) #only second and third elements are TRUE\nx[index] #returns only second and third elements of the x vector\n\n[1] 5.6 7.2\n\n\n\nFor example, imagine vector gdp vector that holds the GDP per capita values for a list of countries and country vector that holds the corresponding country names. Logical indexing may be very useful, if we want to get names of countries with GDP per capita above or below a certain value:\n\ngdp &lt;- c(69687, 67037, 65111, 52367, 41030, 32946, 29961)\ncountries &lt;- c(\"Qatar\", \"Iceland\", \"USA\",\n               \"Germany\", \"United Kingdom\", \"Italy\", \"Spain\")\ncountries[gdp &gt; 40000]\n\n[1] \"Qatar\"          \"Iceland\"        \"USA\"            \"Germany\"       \n[5] \"United Kingdom\"\n\n\nWe can also use this with multiple critertia, for example index countries with GDP higher than 40000 USD and the UN Human Development Index higher than 0.9.\n\nhdi &lt;- c(0.848, 0.938, 0.920, 0.939, 0.920, 0.883, 0.893)\ncountries[gdp &gt; 40000 & hdi &gt; 0.9]\n\n[1] \"Iceland\"        \"USA\"            \"Germany\"        \"United Kingdom\"",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basics of R Programming</span>"
    ]
  },
  {
    "objectID": "02-basics.html#sorting",
    "href": "02-basics.html#sorting",
    "title": "Basics of R Programming",
    "section": "Sorting",
    "text": "Sorting\nOn many occasions, it’s useful to sort a vector to see it’s highest or lowest values. This can be achieved by using the sort function.\n\nnumbers &lt;- c(8, 4, 5, 10, 2, 123)\nsort(numbers)\n\n[1]   2   4   5   8  10 123\n\n\nBy default, R sorts vectors in an increasing order (in case of character vectors, this translates to A-Z sorting). However, the sort function has an additional argument, decreasing, that can be used to specify whether the sorting should be done in the decreasing order. The argument is a default argument, i.e. takes a certain value unless specified otherwise by the user. This is common in R and a lot of functions allow customizing the way they work by specifying additional arguments, which have a default value to avoid the effort of specifying them every time a certain function is used. Such default arguments can easily be recognized in R documentation. In case of sort, the Usage section reads sort(x, decreasing = FALSE, ...). This means, that the function takes x (the vector to be sorted) as its main argument, and decreasing, which defaults to FALSE. The argument decreasing is also logical - can only take TRUE or FALSE values - this is a common argument type if a certain operation can be performed in two different ways of with an additional element that may not always be desired.\n\nsort(numbers, decreasing = TRUE)\n\n[1] 123  10   8   5   4   2\n\n\nWhile sorting a vector may be useful in certain circumstances, a lot of the time we may actually need to sort the values by another vector. For example, let’s assume that we have a vector of names and corresponding ages, and we want to see the names ordered by the age.\n\nnames &lt;- c(\"Thomas\",\"Peter\",\"Ahmed\",\"Emily\",\"Helena\")\nage &lt;- c(50, 10, 20, 15, 40)\n\nThis can be achieved using the order function, which returns indices of the vector needed to re-arrange it into sorted order.\n\norder(age)\n\n[1] 2 4 3 5 1\n\n\nIn this case, the age of 10 (index 2) should go to the first place, 15 (index 4) to the second position, 20 (index 3) to the third, etc. Note that the two following operations are equivalent:\n\nsort(age)\n\n[1] 10 15 20 40 50\n\nage[order(age)]\n\n[1] 10 15 20 40 50\n\n\nThe first one tells R to simply sort the values of age, whereas the second to index age by the indices of age in a sorted order. To get the names sorted by age, we can use:\n\nnames[order(age)]\n\n[1] \"Peter\"  \"Emily\"  \"Ahmed\"  \"Helena\" \"Thomas\"\n\nnames[order(age, decreasing = TRUE)] #decreasing order\n\n[1] \"Thomas\" \"Helena\" \"Ahmed\"  \"Emily\"  \"Peter\" \n\n\nFinally, the rank function returns the sample ranks of a given vector, i.e. their relative position in a sorted list. Note that this is different from order. rank returns the position corresponding to each value in a sorted order, whereas order returns indices of the original vector needed to put it in a sorted order.\n\nage\n\n[1] 50 10 20 15 40\n\nrank(age)\n\n[1] 5 1 3 2 4\n\norder(age)\n\n[1] 2 4 3 5 1\n\n\nSo in our example, the first value of the vector returned by rank(age) is 5, since the first value of the age vector is 50, which would be last in the numeric order. The first value of the vector returned by order(age) is 2 - this is because, the 2nd element of age (i.e. the value of 10) should go to the first position for the vector to me correctly ordered.\nFinally, logical indices can be converted into numerical values using the which function. It takes a logical vector as input and returns the indices at which the value of the vector is TRUE. You can see an example below:\n\nnumbers &lt;- 1:10\nnumbers &gt; 5\n\n [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nwhich(numbers &gt; 5)\n\n[1]  6  7  8  9 10\n\n\nThis function is helpful in some certain situations, however it’s a bad practice to apply it in cases when logical indexing is sufficient, for example:\n\nnumbers[numbers &gt; 5]\n\n[1]  6  7  8  9 10\n\n\nis sufficient, and there’s no need to use:\n\nnumbers[which(numbers &gt; 5)]\n\n[1]  6  7  8  9 10\n\n\nOne of the situations in which the use of which function can be preferred to simple logical indexing is when our vector contains missing values (discussed in the next chapter. For example, the first expression will return NA.\n\nnumbers &lt;- c(2, 4, 1, 10, 20, NA)\nnumbers[numbers &gt; 5]\n\n[1] 10 20 NA\n\n\nThis is because, running logical comparisons such as numbers &gt; 5 always returns missing values, along the TRUE and FALSE logical values. This should make sense, since NA is not comparable to any number.\n\nnumbers &gt; 5\n\n[1] FALSE FALSE FALSE  TRUE  TRUE    NA\n\n\nwhich skips the NA values, only returning the indices of values that are TRUE.\n\nwhich(numbers &gt; 5)\n\n[1] 4 5\n\n\nAs a result, we can perform indexing on variable with missing values using which:\n\nnumbers[which(numbers &gt; 5)]\n\n[1] 10 20\n\n\nTwo cousins of the which function are which.max and which.min, which return the index of the highest and lowest value in a vector. So, coming back to the ages example, we can retrieve the name of the person with highest and lowest age using respectively:\n\nnames[which.max(age)]\n\n[1] \"Thomas\"\n\nnames[which.min(age)]\n\n[1] \"Peter\"",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basics of R Programming</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html",
    "href": "03-data_structures.html",
    "title": "Data Structures",
    "section": "",
    "text": "Introduction\nIn the previous chapter you have become familiar with the most common data structure in R programming - a vector. In this section, you will be introduced to some more advanced data structures that are often used in R, in particular the data.frame, which is the most common way of storing and manipulating data in R.\nGenerally, the best way to examine any R object is using the str() function, which returns contents of the object along with its class. For example you can check how it works for simple vectors\nnumbers &lt;- c(5, 3, 8)\nstr(numbers)\n\n num [1:3] 5 3 8\nnum indicates that it is an integer vector and [1:3] tells us that its index ranges from 1 to 3.\nwords &lt;- c(\"five\",\"three\",\"eight\")\nstr(words)\n\n chr [1:3] \"five\" \"three\" \"eight\"",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#data-frames",
    "href": "03-data_structures.html#data-frames",
    "title": "Data Structures",
    "section": "Data Frames",
    "text": "Data Frames\nIn the previous chapter’s exercises you’ve manipulated data related to some basic development indicators of several countries. When we’re dealing with multiple variables represented by multiple vectors, it’s often very useful to store them toegether as one entity - a data.frame. Data frames can simply be thought of as tables, where each of the columns is a vector with its unique name. In this case, we can store the information about countries in a data frame called dev_data.\n\ncountry &lt;- c(\"Argentina\", \"Georgia\", \"Mexico\", \n             \"Philippines\", \"Turkey\", \"Ukraine\")\neys &lt;- c(17.6, 15.4, 14.3, 12.7, 16.4, 15.1)\nmys &lt;- c(10.6, 12.8, 8.6, 9.4, 7.7, 11.3)\nlexp &lt;- c(76.5, 73.6, 75, 71.1, 77.4, 72)\ngni &lt;- c(17611, 9570, 17628, 9540, 24905, 7994)\ndev_data &lt;- data.frame(country, eys, mys, lexp, gni)\n\nWe can use the head function to see the first 5 rows of the data (in the toy example we have above it might seem unnecessary, but it is useful to get an overview of all the variables when the data consists of potentially thousands of rows).\n\nhead(dev_data)\n\n      country  eys  mys lexp   gni\n1   Argentina 17.6 10.6 76.5 17611\n2     Georgia 15.4 12.8 73.6  9570\n3      Mexico 14.3  8.6 75.0 17628\n4 Philippines 12.7  9.4 71.1  9540\n5      Turkey 16.4  7.7 77.4 24905\n6     Ukraine 15.1 11.3 72.0  7994\n\n\nThe str() function is also very useful to get an overview of the variables included in a dataframe:\n\nstr(dev_data)\n\n'data.frame':   6 obs. of  5 variables:\n $ country: chr  \"Argentina\" \"Georgia\" \"Mexico\" \"Philippines\" ...\n $ eys    : num  17.6 15.4 14.3 12.7 16.4 15.1\n $ mys    : num  10.6 12.8 8.6 9.4 7.7 11.3\n $ lexp   : num  76.5 73.6 75 71.1 77.4 72\n $ gni    : num  17611 9570 17628 9540 24905 ...\n\n\nTo access a column stored in a dataframe, you can use the $ operator.\n\ndev_data$gni\n\n[1] 17611  9570 17628  9540 24905  7994\n\n\nSimilarily, we can use the same operator to create a new column:\n\ndev_data$log_gni &lt;- log(dev_data$gni)\ndev_data$log_gni\n\n[1]  9.776279  9.166388  9.777244  9.163249 10.122824  8.986447\n\n\nAs in the case of vectors, data frames can be indexed to retrieve values stored at specific positions. Since data frame is a table, each position in a dataframe is associated with two indices - one for rows, the other for columns - the first index references the row and the second the column. For example, the code below retrieves the value from the second row from the third column of dev_data.\n\ndev_data[2, 3]\n\n[1] 12.8\n\n\nNote that this is identical to:\n\ndev_data$mys[2]\n\n[1] 12.8\n\n\nThis is because the mys is the third column in dev_data.\nBy leaving one of the indices empty, we can also retrieve entire row/column of a data frame:\n\ndev_data[1, ] #get first row\n\n    country  eys  mys lexp   gni  log_gni\n1 Argentina 17.6 10.6 76.5 17611 9.776279\n\ndev_data[, 2] #get second column\n\n[1] 17.6 15.4 14.3 12.7 16.4 15.1\n\n\nData frames can also be indexed with integer vectors. Such indexing will always return a smaller data frame. For example, to retrieve rows 1 and 5 from columns 2 and 3, we can do:\n\ndev_data[c(1,5), c(2,3)]\n\n   eys  mys\n1 17.6 10.6\n5 16.4  7.7\n\n\nSimilarily, character vectors referencing the column names can be used to subset a dataframe. To achieve similar result to the one above, one could also type:\n\ndev_data[c(1,5), c(\"eys\",\"mys\")]\n\n   eys  mys\n1 17.6 10.6\n5 16.4  7.7\n\n\nWe can also use logical indexing to subset dataframes. Recall from the previous chapter, that we can check which values of a given vector satisfy a certain condition by:\n\ndev_data$gni &gt; 10000\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\n\nWe can then use the output generated by the above code to index the dev_data data frame and obtain the rows with gni per capita larger than 10000:\n\ndev_data[dev_data$gni &gt; 10000, ]\n\n    country  eys  mys lexp   gni   log_gni\n1 Argentina 17.6 10.6 76.5 17611  9.776279\n3    Mexico 14.3  8.6 75.0 17628  9.777244\n5    Turkey 16.4  7.7 77.4 24905 10.122824\n\n\nThere are many useful functions that work in combination with data frames. Below, there are several examples:\n\nis.data.frame(dev_data) #check if an object is of class `data.frame`\n\n[1] TRUE\n\nnrow(dev_data) #number of rows\n\n[1] 6\n\nncol(dev_data) #number of columns\n\n[1] 6\n\ncolnames(dev_data) #column names\n\n[1] \"country\" \"eys\"     \"mys\"     \"lexp\"    \"gni\"     \"log_gni\"\n\nrownames(dev_data) #row names\n\n[1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n\nThe with command allows to evaluate column names in the context of a given data frame. This means, that we do not have to reference the data frame name whenever we use one of its columns. Suppose we wanted to calculate the UN’s Education Index as in the previous section’s exercises and assign it’s values to a new column in dev_data, dev_data$edu_ind. This could be done by:\n\ndev_data$edu_ind &lt;- (dev_data$mys / 15 + dev_data$eys / 18)/2\n\nHowever, in many circumstances this will require you to reference the name of the data frame you are using multiple times, often making the code long and unreadable. To avoid it, it’s often useful to do:\n\ndev_data$edu_ind &lt;- with(dev_data, (mys / 15 + eys / 18)/2)\n\nThe with function takes name of the dataframe as its first argument and the operation you want to perform as the second argument.\nSimilarily, to subset a dataframe by multiple variables, the subset() command can be used:\n\ndev_data[dev_data$eys &gt; 15 & dev_data$lexp &gt; 75, ]\n\n    country  eys  mys lexp   gni   log_gni   edu_ind\n1 Argentina 17.6 10.6 76.5 17611  9.776279 0.8422222\n5    Turkey 16.4  7.7 77.4 24905 10.122824 0.7122222\n\nsubset(dev_data, eys &gt; 15 & lexp &gt; 75)\n\n    country  eys  mys lexp   gni   log_gni   edu_ind\n1 Argentina 17.6 10.6 76.5 17611  9.776279 0.8422222\n5    Turkey 16.4  7.7 77.4 24905 10.122824 0.7122222",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#factors",
    "href": "03-data_structures.html#factors",
    "title": "Data Structures",
    "section": "Factors",
    "text": "Factors\nWhen looking at the str(dev_data) you could’ve noticed that the country variable is a vector type that we haven’t encountered earlier - a factor. Factors are a specific type of vectors used to store values that take a prespecified set of values, called factor levels. For example, suppose we have two character vectors storing names of students and their year. We can use factor() to create a factor vector from a character vector. This can be done for any other type of vector as well.\n\nname &lt;- c(\"Thomas\",\"James\",\"Kate\",\"Nina\",\"Robert\",\"Andrew\",\"John\")\nyear_ch &lt;- c(\"Freshman\",\"Freshman\",\"Junior\",\"Sophmore\",\"Freshman\",\"Senior\",\"Junior\")\nyear_ch\n\n[1] \"Freshman\" \"Freshman\" \"Junior\"   \"Sophmore\" \"Freshman\" \"Senior\"   \"Junior\"  \n\nyear &lt;- factor(year_ch)\nyear\n\n[1] Freshman Freshman Junior   Sophmore Freshman Senior   Junior  \nLevels: Freshman Junior Senior Sophmore\n\n\nWe can view the unique levels of the factor using the levels() function:\n\nlevels(year)\n\n[1] \"Freshman\" \"Junior\"   \"Senior\"   \"Sophmore\"\n\n\nA crucial difference between factor and character vectors is that the former have an underlying integer representation. That means, that there’s a natural ordering to their levels, which is alphabetic by default. We can see that using the coercion function as.numeric on the year factor.\n\nyear\n\n[1] Freshman Freshman Junior   Sophmore Freshman Senior   Junior  \nLevels: Freshman Junior Senior Sophmore\n\nas.numeric(year)\n\n[1] 1 1 2 4 1 3 2\n\n\nNote that the ordering of the values corresponds with the ordering obtained by the levels() function. This matters in some circumstances (such as when using factor variables in regression models, discussed in the Linear Regression section of the course). It’s a good practice to explicitly pass the factor levels to the factor() constructor. For example, in our case, “Sophmore” comes as the last value of the factor, even though it would make more sense for it to be second. Explicit creation of the factor levels can be seen below:\n\nyear_ch &lt;- c(\"Freshman\",\"Freshman\",\"Junior\",\n          \"Sophmore\",\"Freshman\",\"Senior\",\"Junior\")\nyear &lt;- factor(year_ch, levels = c(\"Freshman\",\"Sophmore\",\"Junior\",\"Senior\"))\n\nWe can now see that the ordering of the levels is different, and so is the underlying numeric representation of the factor:\n\nlevels(year)\n\n[1] \"Freshman\" \"Sophmore\" \"Junior\"   \"Senior\"  \n\nas.numeric(year)\n\n[1] 1 1 3 2 1 4 3\n\n\nNote that we cannot change the value of a factor vector to any other than the pre-specified levels:\n\nyear[1] &lt;- \"Graduate\"\n\nThe error message returned by R means that the value we were trying to assign to the factor is not one of the predefined levels (i.e. “Freshman”,“Junior”, “Senior” and “Sophmore”) and thus NA missing value was generated.\nHowever, if we know that a level that has no values attached to it will be created in the future, NAs can be avoided by explicitly creating an unused levels when constructing the factor vector.\n\nyear_ch &lt;- c(\"Freshman\",\"Freshman\",\"Junior\",\n          \"Sophmore\",\"Freshman\",\"Senior\",\"Junior\")\nyear &lt;- factor(year_ch, levels = c(\"Freshman\",\"Sophmore\",\"Junior\",\"Senior\", \"Graduate\"))\n\nHere, we have created the variable with 5 levels: Freshman, Sophmore, Junior, Senior, Graduate, even though only 4 of them are actual values of the factor. As a result, we can assign a value with the “Graduate” value without producing NAs. The relevance of having empty factor levels will become apparent in the next part of the book when discussing Cross-Tabulation.\n\nyear[1] &lt;- \"Graduate\"\nyear\n\n[1] Graduate Freshman Junior   Sophmore Freshman Senior   Junior  \nLevels: Freshman Sophmore Junior Senior Graduate\n\n\nWe can also rename the levels of an existing factor, by using the levels&lt;- command. This can be done either to specific levels of a factor…\n\nyear &lt;- factor(year_ch, levels = c(\"Freshman\",\"Sophmore\",\"Junior\",\"Senior\"))\nlevels(year)[1] &lt;- \"Fresher\"\nyear\n\n[1] Fresher  Fresher  Junior   Sophmore Fresher  Senior   Junior  \nLevels: Fresher Sophmore Junior Senior\n\n\n…or to all the levels:\n\nlevels(year) &lt;- c(\"First\",\"Second\",\"Third\",\"Final\")\nyear\n\n[1] First  First  Third  Second First  Final  Third \nLevels: First Second Third Final\n\n\nThis way, all the values of the character are changed very quickly.\nFinally, there’s some confusion about the difference between factor() and as.factor() functions. In many contexts, these can be used equivalently, since both create a factor vector from a numeric or a character vector. However, some important differences include:\n\nfactor() allows to explicitly pass vector levels at construction, whether as.factor() assigns them by default\nThe behaviour of the two functions is different when passed factors with empty levels. For example, let’s create the year factor as earlier and only keep the first three values. In this case, the Sophmore and Senior levels are unused.\n\n\nyear_char &lt;- c(\"Freshman\",\"Freshman\",\"Junior\",\n          \"Sophmore\",\"Freshman\",\"Senior\",\"Junior\")\nyear &lt;- factor(year_char, levels = c(\"Freshman\",\"Sophmore\",\"Junior\",\"Senior\"))\nyear &lt;- year[1:3]\nyear\n\n[1] Freshman Freshman Junior  \nLevels: Freshman Sophmore Junior Senior\n\n\nPassing the year vector to as.factor will not change anything in the vector’s structure:\n\nas.factor(year)\n\n[1] Freshman Freshman Junior  \nLevels: Freshman Sophmore Junior Senior\n\n\nHowever, using factor() constructor on an existing factor vector is a convenient way to drop unused levels (when it’s desirable):\n\nfactor(year)\n\n[1] Freshman Freshman Junior  \nLevels: Freshman Junior\n\n\n\nThe performance of as.factor() tends to be quicker when numeric or character vectors are passed to it. The two commands also treat NA levels slightly differently. You can read more about it in this Stack Overflow post.\n\nFinally, some R functions such as the data.frame constructor treat all read all character vectors as factors by default. This can be noticed by examining the dev_data data frame we created earlier:\n\nstr(dev_data)\n\n'data.frame':   6 obs. of  7 variables:\n $ country: chr  \"Argentina\" \"Georgia\" \"Mexico\" \"Philippines\" ...\n $ eys    : num  17.6 15.4 14.3 12.7 16.4 15.1\n $ mys    : num  10.6 12.8 8.6 9.4 7.7 11.3\n $ lexp   : num  76.5 73.6 75 71.1 77.4 72\n $ gni    : num  17611 9570 17628 9540 24905 ...\n $ log_gni: num  9.78 9.17 9.78 9.16 10.12 ...\n $ edu_ind: num  0.842 0.854 0.684 0.666 0.712 ...\n\n\nAs you can see country is a factor with 6 levels - each for one country name. This doesn’t make too much sense, as the column is unlikely to have any repeating values. To avoid this behaviour, we can set the stringsAsFactors optional argument in the data.frame function explicitly to FALSE. This way, all the character vectors remain character variables in the data frame.\n\ndev_data &lt;- data.frame(country, eys, mys, lexp, gni, stringsAsFactors = FALSE)\nstr(dev_data)\n\n'data.frame':   6 obs. of  5 variables:\n $ country: chr  \"Argentina\" \"Georgia\" \"Mexico\" \"Philippines\" ...\n $ eys    : num  17.6 15.4 14.3 12.7 16.4 15.1\n $ mys    : num  10.6 12.8 8.6 9.4 7.7 11.3\n $ lexp   : num  76.5 73.6 75 71.1 77.4 72\n $ gni    : num  17611 9570 17628 9540 24905 ...",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#reading-and-writing-the-data",
    "href": "03-data_structures.html#reading-and-writing-the-data",
    "title": "Data Structures",
    "section": "Reading and writing the data",
    "text": "Reading and writing the data\n\nReading from CSV\nWhile so far, we’ve created small and simple datasets by manually typing them into the scripts, the usual way of loading data into R is through external files. The most common format used to store data for R analysis is a CSV file, which stands for Comma Separated Values. This essentially means, that the data is represented as a text file, in which values are separeted by columns to indicate their relative positions - for example, a csv file with 5 columns will have 4 commas to separate them in each row.\nIn the example below, we read in data on Human Development Indicators for 209 countries for 2018 obtained from the UN Human Development Reports. Yuo can download the file used in the example from here.\n\ndev &lt;- read.csv(\"data/un_data/dev2018.csv\", stringsAsFactors = FALSE)\n\nIn the above example, the first argument specifies the path to the file read as a string, i.e. enclosed in quotation marks. The file can be read: 1. using absolute path - for example dev &lt;- read.csv(\"C:/Users/yourusername/Documents/dev2018.csv\") in Windows or dev &lt;- read.csv(\"/Users/yourusername/Documents/dev2018.csv\") in MacOS. In this case, you need to provide the full path to where the file is located in the computer.\n\nusing relative path, as in the above example. In this case, R will search for the directory in your current working directory. Working directory is simply the specific folder in your computer in which R looks for the data. R Studio usually sets one default working directory (this can be changed under Tools -&gt; Global Options -&gt; Set Default Working Directory). This means that every time you open RStudio or restart your R session (as described in Chapter 1, the working directory is set to this default. You can also change working directory manually by executing the setwd() function from your script or the console.\n\n\nsetwd(\"C:/Users/yourusername/folder\")\n\nYou can also get your current working directory by using the getwd() function:\n\ngetwd()\n\nWhile some users tend to include setwd(path/to/project) in the beginnings of their scripts, this is potentially problematic, as whenever you move your data or script to another folder, errors are likely to occur. Therefore, it is a good practice to always set working directory to the location of your R Source script and keep the data in the same folder as your source script. This can be done by choosing the Session tab\nNote that in this case, it is assumed that you have selected “Set Working Directory” &gt; “To Source File” location from the “Session” tab in Rstudio, as discussed in the Introduction and that the directory of the source file has a folder called “data” in which the dev2018.csv file is stored. Alternatively, dev &lt;- read.csv(\"dev2018.csv\") would read the file directly from your working directory. You could also use dev &lt;- read.csv(\"C:/Users/yourusername/Documents/dev2018.csv\") in Windows or dev &lt;- read.csv(\"/Users/yourusername/Documents/dev2018.csv\") in MacOS to read the data file from an arbitary folder using its absolute path. Similarily to the data.frame constructor, we can also use the stringsAsFactors argument to ensure all character variables are read as strings.\nYou can also save data to .csv files by using the write.csv, which takes the data frame as its first argument and the string specifying the path to which you want to save the file as the second argument. For example, suppose we want to keep only the first 40 rows of the data and store it in a separate file.\n\ndev_new &lt;- dev[1:40, ]\nwrite.csv(dev_new, \"data/un_data/dev_new.csv\")\n\n\n\nReading from other formats\nWhile csv is the most common format, the data is often likely to come in many other variants - common examples include Stata’s .dta files or SPSS’ .sav, as well as .xlsx Excel format. Some of the R packages offer functionalities percisely to deal with such files.\nSo far, we have only used the built-in functionalities offered by R. While their range is pretty extensive and the ones covered in this course are only the tip of the iceberg, much more than that is offered by user-made packages, which offer new functions useful for specific tasks. The official R packages are available through CRAN. To use a package it needs to be installed first and then loaded. For example, to use an example package named foo, you should first run install.packages(\"foo\") to download the package files from CRAN and install it and then put library(foo) in your R Script to load it into R. Note that while installation has to be done only once, you have to load the library every time you use it - that’s why, you should always put the library calls at the top of your R script. If you use a function from that package without loading it first, your R script execution will fail! Please also note, that you pass the package name as a string (i.e. in quotation marks) to the install.packages, but without them to library.\n\ninstall.packages(\"haven\")\n\nComing back to our example, we can use the R package haven to load Stata, SPSS and SAS files. You can see an example below:\n\nlibrary(haven)\ndev_stata &lt;- read_dta(\"data/un_data/dev2018.dta\")\n\nSimilarily, the data can be written using:\n\nwrite_dta(dev, \"data/un_data/dev2018.dta\")\n\nOther alternatives offered by the haven package inlcude read_sav or read_xpt. Other packages useful for reading unusual data types include readxl for reading Excel files and foreign for a broader choice of file types.",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#missing-values",
    "href": "03-data_structures.html#missing-values",
    "title": "Data Structures",
    "section": "Missing values",
    "text": "Missing values\nAs mentioned earlier, the NA missing value constant is particularly important in R. Real-life data that you are likely to deal with most of the time when using R in practice is often imperfect and missingness should be addressed as one of the first steps of the analysis process.\nis.na() command can be used to determine whether a value of an R object is missing. It returns true for each value of the index which is missing.\n\nnumbers &lt;- c(1, 4, NA, 6, NA)\nis.na(numbers)\n\n[1] FALSE FALSE  TRUE FALSE  TRUE\n\n\nTo count NAs in an R object we can levarage the fact that TRUE values are also interpreted as 1 and use the sum function:\n\nsum(is.na(numbers))\n\n[1] 2\n\n\nYou can also verify whether an object contains NAs using the anyNA function. Let’s check if the HDI data we have loaded contains any missing values:\n\ndev &lt;- read.csv(\"data/un_data/dev2018.csv\")\nanyNA(dev)\n\n[1] TRUE\n\n\nThe column returns TRUE. Therefore there is some missingness in the data.\nAnother useful function for missing data analysis is complete.cases. As the name suggests, given a data frame it returns a logical vector with TRUE for each row which doesn’t contain missing values. We can verify which observations are the cause of the data missingness:\n\ndev[!complete.cases(dev), ]\n\n                                country  eys   gni lexp mys\n91  Korea (Democratic People's Rep. of) 10.8    NA 72.1  NA\n122                               Nauru 11.3 17313   NA  NA\n150                          San Marino 15.1    NA   NA  NA\n180                              Tuvalu 12.3  5409   NA  NA\n195                             Somalia   NA    NA 57.1  NA",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#lists",
    "href": "03-data_structures.html#lists",
    "title": "Data Structures",
    "section": "Lists",
    "text": "Lists\nThe final key R data structure covered in this section are lists. Similarily to data frames, lists can be thought of containers to store other data structures.1 However, unlike data frames, they are less strict in terms of their contents - a list can store vectors of different length, data frames and even other lists. Lists are created with the list() constructor.\n\nmy_list &lt;- list(names = c(\"Tom\",\"James\",\"Tim\"), values = 1:20)\nmy_list\n\n$names\n[1] \"Tom\"   \"James\" \"Tim\"  \n\n$values\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\nYou can extract elements from a list using their names or their numeric index. To index a list, double square brackets [[ are used, as opposed to vectors.\n\nmy_list[[1]]\n\n[1] \"Tom\"   \"James\" \"Tim\"  \n\n\n\nmy_list[[\"names\"]]\n\n[1] \"Tom\"   \"James\" \"Tim\"  \n\n\nIf a list is indexed with single brackets, it returns a one-element list, rather than the object stored in it:\n\nvalues &lt;- my_list[[\"values\"]]\nstr(values)\n\n int [1:20] 1 2 3 4 5 6 7 8 9 10 ...\n\nvalues &lt;- my_list[\"values\"]\nstr(values)\n\nList of 1\n $ values: int [1:20] 1 2 3 4 5 6 7 8 9 10 ...\n\n\nYou can also extract elements from a list using the $ operator, similarily to data.frames. Finally, you can assign values to lists similarily as in the case of vectors or data.frames:\n\nmy_list[[\"new\"]] &lt;- c(\"new\",\"values\")\nstr(my_list)\n\nList of 3\n $ names : chr [1:3] \"Tom\" \"James\" \"Tim\"\n $ values: int [1:20] 1 2 3 4 5 6 7 8 9 10 ...\n $ new   : chr [1:2] \"new\" \"values\"",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "03-data_structures.html#footnotes",
    "href": "03-data_structures.html#footnotes",
    "title": "Data Structures",
    "section": "",
    "text": "More specifically, the data.frame class is a special type of list - you can verify that by running the typeof function with a data frame as input.↩︎",
    "crumbs": [
      "THE BASICS",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Structures</span>"
    ]
  },
  {
    "objectID": "04-exploratory_analysis.html",
    "href": "04-exploratory_analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Introduction\nNext to data cleaning, exploratory data analysis is one of the first steps taken in the process of analysing quantitative data of any kind. Essentially, it refers to getting an overview of the data through looking at simple summary statistics and plots to understand the distribution of each variable, as well as look for particularly obvious and pronounced relationships between the variables. If one is taking a deductive approach, with a clear-cut, falsifiable hypothesis about the data defined upfront (such as higher per capita income is related to higher levels of democracy or income equality increases levels of subjective well-being), exploratory data analysis helps you verify whether the hypothetized relationship is in the data at all - for example by applying the so-called Inter-Ocular Trauma Test (if it hits you between the eyes, it’s there!) to a plot. This informs further formal statistical analyses. It also allows you to identify factors that may be important for the hypothesized relationship and should be included in the formal statistical model. In case of an inductive approach, exploratory data analysis allows you to find patterns and form hypothesis to be furhter tested using formal statistical methods.\nIn this chapter, we will cover some basic exploratory methods that can be applied to examine numeric and categorical data. For this purpose we will use the data from UCI Machine Learning Repository, which covers math grades achieved in three years of education by a sample of students, along with some demographic variables 1. The data download link is available at the top of this course page. To load it, we can use the familiar read.csv function. Note that in this case, the sep optional argument is specified to \";\". You can find out why by reading about this argument in the function documentation (?read.csv) and by examining the dataset using your computer’s notepad app.\nmath &lt;- read.csv(\"data/student/student-mat-data.csv\", sep = \";\")",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "04-exploratory_analysis.html#numeric-variables",
    "href": "04-exploratory_analysis.html#numeric-variables",
    "title": "Exploratory Data Analysis",
    "section": "Numeric variables",
    "text": "Numeric variables\n\nHistograms\nThe simplest and often most powerful way to examine a single numeric variable is through the use of histogram. Histogram divides a variable in ranges of equal size called bins. Each bin is then represented as a bar, the height of which corresponds with the count/proportion of the observations falling in that range. The main difference between a histogram and a bar chart is that a histogram does not have breaks between the bars, because the variable it describes is assumed to be continuous, not discrete. Let’s examine two numeric variables from the math dataset - age and absences (total absent hours recorded by the teacher for each student) using histograms. A histogram is created using the hist function:\n\nhist(math$age, breaks = length(unique(na.omit(math$age))))\n\n\n\n\n\n\n\n\n\nhist(math$absences)\n\n\n\n\n\n\n\n\nIn both cases, we can see that the lowest values are the most frequent. For example, from the second histogram we can read that 250 of the 395 students in the samples were absent between 0-5 hours during the school year.\n\n\nMean\nMean (aka average) is the simplest statistic describing any numeric variable - it is simply the sum of a variable/vector divided by its length. In R, we can calculate the mean of any variable using the mean function. For example, let’s examine the average number of absences in the sample:\n\nsum(math$absences)/nrow(math)\n\n[1] 5.787013\n\nmean(math$absences)\n\n[1] 5.787013\n\n\nThe same done for age returns this value:\n\nmean(math$age)\n\n[1] 16.68831\n\n\nHad we not removed the missing values (NA's) at the start, this command would not have worked and would have returned NA. Should you ever come across this issue, then this is a reminder that you need to do something with your missing data. If you just wish to circumvent the problem for the time being, you could call:\n\nmean(math$age, na.rm = TRUE)\n\n[1] 16.68831\n\n\nwhere we specify the argument na.rm(remove NA values) to TRUE.\nWe can also use the trim argument from to specify the fraction of observations to be removed from each end of the sorted variables before calculating the mean. This makes our estimate of the mean more robust to potentially large and unrepresentative values affecting the calculated value - so-called outliers, which will be discussed more extensively in the section on quantiles. Note that specifying the trim argument to 0.1 doesn’t seem to change the mean of age significantly:\n\nmean(math$age, na.rm = TRUE, trim = 0.1)\n\n[1] 16.61812\n\n\nHowever, doing the same in case of absences changes the value of average absences quite a lot. Can you think of the reason why? Take a look at the histograms of both variables.\n\nmean(math$absences, trim = 0.1)\n\n[1] 4.307443\n\n\n\n\nVariance and standard deviation\nWhile a mean offers a good description of the central tendency of a variable (i.e. a value that we would expect to see most often), describing a variable just by its mean can be very misleading. For example, consider the values -10000, 20, 10000 and 15, 20, 25. In both cases the mean is the same:\n\nx &lt;- c(15, 20, 25)\ny &lt;- c(-985, 20, 1025)\nmean(x) == mean(y)\n\n[1] TRUE\n\n\nHowever, it would be very misleading to say that these variables are similar. We could try to describe this difference by computing the average distance between each value and the mean:\n\nmean(x - mean(x))\n\n[1] 0\n\nmean(y - mean(y))\n\n[1] 0\n\n\nHowever, this results in a 0, since the negative and positive values in our example cancel each other out. To avoid this, we can measure the variance, which calculates the mean of the sum of the squared distances between each value of a variable and its mean. Since the distance is squared (always positive), the positive and negative values will not cancel out.\n\nmean((x - mean(x))^2)\n\n[1] 16.66667\n\nmean((y - mean(y))^2)\n\n[1] 673350\n\n\nWe can see that this captures the difference between our two vectors.\nYou can calculate the variance with a simple shortcut in R, with the var function:\n\nvar(x)\n\n[1] 25\n\nvar(y)\n\n[1] 1010025\n\n\nNote that this gives us different results than our variance computed by hand. This is because we calculate the population wariance in which we divide by the number of observations in the population (or length of the vector), N. So, the variance we calculated “manually” above is equivalent to:\n\nsum((x - mean(x))^2)/length(x)\n\n[1] 16.66667\n\nmean((x - mean(x))^2)\n\n[1] 16.66667\n\n\nInstead, the var() function calculates the sample variance, for which we divide the sum of squared distances from the mean by \\(N-1\\). This is because dividing the sample by N tends to underestimate the variance of the population. The mathematical reasons behind it are clearly outlined in this article. So, we can “manually” arrive at equivalent estiamte to the one obtained using the var function by:\n\nsum((x - mean(x))^2)/(length(x) - 1) == var(x)\n\n[1] TRUE\n\n\nWe can apply the variance function to absence and age, to see their spread:\n\nvar(math$age, na.rm = TRUE)\n\n[1] 1.631764\n\nvar(math$absences)\n\n[1] 65.12119\n\n\nOne problem arising when using variance to describe the data is that its units aren’t interpretable, since they are squared. Therefore, saying that the variance of the absence time is 64 squared hours doesn’t sound too intuitive. To avoid this, we usually use use the standard deviation in practice, which is simply the square root of the variance. Through taking the square root we return the variable to its original units. The standard deviation of a variable is calculated using the sd function:\n\nsd(math$age, na.rm = TRUE)\n\n[1] 1.277405\n\nsd(math$age, na.rm = TRUE) == sqrt(var(math$age, na.rm = TRUE))\n\n[1] TRUE\n\n\nNot ethat I am including the na.rm = TRUE option for illsutrative purposes only, since we did remove missing values, earlier.\nWe can know compare the standard deviation and mean of both variables:\n\nwith(math, c(mean = mean(age, na.rm = TRUE), \n  sd = sd(age, na.rm = TRUE)))\n\n     mean        sd \n16.688312  1.277405 \n\nwith(math, c(mean = mean(absences, na.rm = TRUE), \n  sd = sd(absences, na.rm = TRUE)))\n\n    mean       sd \n5.787013 8.069770 \n\n\nIt can be clearly seen that the hours of students’ absence have more variability than the students’ ages. This makes intuitive sense, since the sample consists of students from roughly the same age group (the easiest way you can see it is by running unique(math$age)). At the same time, students differ match more in the total hours of absence. This explains why the trimmed mean was that different from overall mean in case of absences, yet quite similar for age.\nYou can use the widget below to see how varying the standard deviation and the mean affects the distribution of a variable (in this case a normally distributed random variable). Note that you need an active internet connection for the app to load.\n\n\n\n\n\n\nQuantiles\nThe final statistic we are going to discuss is quantile. Quantiles allow us to get a better grasp of the distribution of the data. Essentially, quantiles are cut points that divide the variable into intervals of equal sizes. For example, deciles are 10-quantiles, dividing the variable into 10 ranges. For example the 8th decile of a variable is the value greater than 80% of the values in this variable. In R we can obtain an arbitrary quantile using the quantile function, specifying the proportion below each of the cutpoints through the probs argument.\n\nquantile(math$absences, probs = 0.9)\n\n90% \n 14 \n\n\nIn the above example, we can see that 90% of the values of the variable absences are lower than 14. The probs argument can be either a scalar or a vector, so we can obtain multiple quantiles at once. For example in the example below we obtain so-called quartiles (4-quantiles).\n\nquantile(math$absences, probs = c(0, .25, .5, .75, 1)) #quartiles\n\n  0%  25%  50%  75% 100% \n   0    0    4    8   75 \n\n\nWe could get deciles by:\n\nquantile(math$absences, probs = seq(0, 1, by = 0.1))\n\n  0%  10%  20%  30%  40%  50%  60%  70%  80%  90% 100% \n   0    0    0    2    2    4    4    6   10   14   75 \n\n\nWe can visualize this using a histogram:\n\n\n\n\n\n\n\n\n\n\n\nMedian\nThe median is a specific quantile - the 50th percentile of a variable, i.e. the midpoint of the variable’s distribution. As opposed to the mean, it’s not affected by outlying values. Large differences between mean and meadian are often evidence of a skew in the variable’s distribution.\n\n\n\n\n\n\n\n\n\n\n\nOutliers\nFinally, as mentioned earlier, quantiles are particularly useful when it comes to identifying outliers. Outliers are observations with extreme values, lying far from majority of values in a dataset. In some cases they may be results of data collection error, while in others they are simply rare examples of our variable of interest taking a very high or low value. Outliers can often extert high levarage on a given statistic we are measuring (such as the mean), and removing them may sometimes change the results of our analysis significantly. Thus, it is often worth removing them and re-running the analysis to make sure that it’s not affected too severly by a small number of observations with extreme values. Note that this is not to say that outliers should always be removed or disregarded - contrary to that, observations with outlying values should be treated with extra care and it is the role of the analyst to examine why are these values extreme and what are the possible implications for the analysis.\nQuantiles can be used to find the outlying observations - for example, by looking at the 0.001 and 0.999 cutpoints, and considering all values below or above to be outiers.\n\n\nBox plots\nBox plots are commonly used to visualize the distribution of a variable. Below, we use the boxplot function to plot a boxplot of the age variable from the math dataset.\n\nboxplot(math$age)\n\n\n\n\n\n\n\n\nThe box in the middle of the plot corresponds with the inter-quartile range of the variable (IQR) - this is the range between the 1st and the 3rd quartile of the variable (which is equivalent to the value range between 25th and 75th percentile). The thick line in the middle corrsponds to the variable’s median (the 2nd quartile/50th percentile). The ‘whiskers’ (i.e. the horizontal lines connected by the dashed line with each end of the box) correspond to the minimum and the maximum values of the variable. The maximum is defined as the largest value in the variable that is smaller than the number 1.5 IQR above the third quartile and the minimum is the lowest value in the variable that is larger than the number 1.5 IQR below the first quartile. Anything above/below the whiskers numbers is considered an outlier and marked with a dot. In the above example we can see that one observation is an outlier, lying significantly above the upper whisker of the boxplot. We can identify this value by plugging in the above formula for the upper whisker (#3rd quartile + \\(1.5IQR\\)) and finding the value that lies above it.\n\nmaximum &lt;- quantile(math$age, 0.75, na.rm = TRUE, names = FALSE) + \n  1.5 * IQR(math$age, na.rm = TRUE)\nmath$age[which(math$age &gt; maximum)]\n\n[1] 22\n\n\nWhile useful under many circumstances, box plots can be deceiving, as two similarily looking box plots can represent very different disttributions. That’s why it is always useful to look at the variable’s histogram as well. This can be seen in the example below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plots\nFinally, a good way to explore the relationship between two numeric variables visually are scatter plots. Scatter plots represent each observation as a marker, with x-axis represnting value of one variable and y-axis of another. Scatter plots are simply created using the plot function.\n\nplot(math$G1, math$G2)\n\n\n\n\n\n\n\n\nIn the example above, we can see that there’s a positive relationship between student’s grade in first year and the grade in the second year. While such plot would not be sufficient to make any strong empirical claims, it is usually a valuable first step in finding statistical regularities in the dataset. More formal ways of measuring association between variables will be discussed in sections on statisical association and linear regression.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "04-exploratory_analysis.html#categorical-variables",
    "href": "04-exploratory_analysis.html#categorical-variables",
    "title": "Exploratory Data Analysis",
    "section": "Categorical variables",
    "text": "Categorical variables\n\nCross-tabulation\nCategorical variables are often best described by frequency tables, which provide the counts of the number of occurrences of each level of the categorical variable.\n\ntable(math$sex)\n\n\n  F   M \n203 182 \n\n\nAdditionally, we can transform this into a table of proportions, rather than frequencies, by using the prop.table function to transform the output of table.\n\nprop.table(table(math$sex))\n\n\n        F         M \n0.5272727 0.4727273 \n\n\nWe can also convert such table into a bar plot, by using the barplot function.\n\nbarplot(table(math$Mjob))\n\n\n\n\n\n\n\n\n\nbarplot(prop.table(table(math$Mjob)),\n        names.arg = c(\"At home\", \"Health\", \"Other\", \"Services\", \"Teacher\"))\n\n\n\n\n\n\n\n\nThe table function can also be used for cross-tabulation - creating a table summarizing the count of observations in the overlap of two categories. In the example we look at the relationship between the reason for choosing the particular school and paid classes attendance.\n\ntable(math$reason, math$paid)\n\n            \n             no yes\n  course     94  48\n  home       52  57\n  other      15  18\n  reputation 50  51\n\n\nIt appears that students who chose the school because of their course preference were less likely to attend extra paid classes than students choosing the school for other reasons.\nThis can be made more apparent if we substitute frequencies with proportions:\n\nprop.table(table(math$reason, math$paid))\n\n            \n                     no        yes\n  course     0.24415584 0.12467532\n  home       0.13506494 0.14805195\n  other      0.03896104 0.04675325\n  reputation 0.12987013 0.13246753\n\n\nNote that in this case, the proportions are calculated with respect to the total count of participants (i.e. they add up to 1). For comparison purposes, it might be useful to look at the proportion with respect to the total of each of the categories. This can be specified by the margin argument. By setting it to 1, we calculate the proportions with respect to the row margins, i.e. divide the counts of individuals in the paid variable by the total of each category of the reason variable.\n\nprop.table(table(math$reason, math$paid), margin = 1)\n\n            \n                    no       yes\n  course     0.6619718 0.3380282\n  home       0.4770642 0.5229358\n  other      0.4545455 0.5454545\n  reputation 0.4950495 0.5049505\n\n\nBy analogy, margin = 2 leads to the division by the column margins, i.e. sums of both categories of the paid variable.\n\nprop.table(table(math$reason, math$paid), margin = 2)\n\n            \n                     no        yes\n  course     0.44549763 0.27586207\n  home       0.24644550 0.32758621\n  other      0.07109005 0.10344828\n  reputation 0.23696682 0.29310345",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "04-exploratory_analysis.html#customizing-visualizations",
    "href": "04-exploratory_analysis.html#customizing-visualizations",
    "title": "Exploratory Data Analysis",
    "section": "Customizing visualizations",
    "text": "Customizing visualizations\nIn the previous sections, we discussed some basic tools for data visualizations in R, such as histograms, scatter plots, box plots or bar charts. R base graphics allows the user to create powerful and great-looking visualizations. However achieving can be quite complicated. Because of that, a dedicated called ggplot2 was created to enable creating good-looking and informative visualziations with much simpler user interface. The data visualization chapter covers this in more detail. However, in case you wanted to start preparing visualizations for other purposes than exploratory data analysis, you might find some of the tips below useful:\n\nChanging axis labels\nIn case of every plot in R you can change the axis labels by using the xlab and ylab arguments:\n\nplot(math$G1, math$G2, xlab = \"Grade in term 1\", ylab = \"Grade in term 2\")\n\n\n\n\n\n\n\n\nYou can also add the title by specifying the main argument:\n\nplot(math$G1, math$G2, \n     xlab = \"Grade in term 1\", ylab = \"Grade in term 2\",\n     main = \"Student grades\")\n\n\n\n\n\n\n\n\nThe color of the objects in the plot can be altered using the col argument:\n\nplot(math$G1, math$G2, \n     xlab = \"Grade in term 1\", ylab = \"Grade in term 2\",\n     main = \"Student grades\",\n     col = \"red\")\n\n\n\n\n\n\n\n\nIt can also be specified as character vector, with a different color for each point:\n\ncol_gender &lt;- rep(\"red\", nrow(math))\ncol_gender[which(math$sex == \"F\")] &lt;- \"blue\"\nplot(math$G1, math$G2, \n     xlab = \"Grade in term 1\", ylab = \"Grade in term 2\",\n     main = \"Student grades\",\n     col = col_gender)\n\n\n\n\n\n\n\n\nTo make it more informative, you can also add a legend using the legend function:\n\ncol_gender &lt;- rep(\"red\", nrow(math))\ncol_gender[which(math$sex == \"F\")] &lt;- \"blue\"\nplot(math$G1, math$G2, \n     xlab = \"Grade in term 1\", ylab = \"Grade in term 2\",\n     main = \"Student grades\",\n     col = col_gender)\nlegend('bottomright', \n       legend = c('Female', 'Male'),\n       col = c('blue', 'red'), pch = 1)\n\n\n\n\n\n\n\n\nYou can also change the limits of each of the axes, by specifying the xlim and ylim arguments\n\ncol_gender &lt;- rep(\"red\", nrow(math))\ncol_gender[which(math$sex == \"F\")] &lt;- \"blue\"\nplot(math$G1, math$G2, \n     xlab = \"Grade in term 1\", ylab = \"Grade in term 2\",\n     main = \"Student grades\", xlim = c(0, 30), ylim = c(0, 30))",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "04-exploratory_analysis.html#footnotes",
    "href": "04-exploratory_analysis.html#footnotes",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "note that the data was slightly modified to include missing values for demonstration purposes↩︎",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "05-key_programming_concepts.html",
    "href": "05-key_programming_concepts.html",
    "title": "Key Programming Concepts",
    "section": "",
    "text": "Introduction\nTo make the data analysis more efficient, it is crucial to understand some of the crucial programming concepts. In the first part of this section we discuss for loops and if statements. These are so-called “control flow statements”, which are common to almost all programming languages. The second part will discuss the creation and basic usage of functions. Finally, the third part will go through the sapply() function family, a common tool used in R to apply functions over objects multiple times.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Programming Concepts</span>"
    ]
  },
  {
    "objectID": "05-key_programming_concepts.html#for-loops",
    "href": "05-key_programming_concepts.html#for-loops",
    "title": "Key Programming Concepts",
    "section": "For loops",
    "text": "For loops\nFor loops are essentially a way of telling the programming language “perform the operations I ask you to do N times”. A for loop in R beginns with an for() statement, which is followed by an opening curly brace { in the same line - this is esentially opening the for-loop. After this, usually in a new line, you place the code which you want to execute. Then, in the last line you close the for loop by another curly brace }. You can execute the for loop by placing the cursor either on the for statement (first line) or the closing brace (last line) and executing it as any other code. Below, you can see the for loop printing the string \"Hello world!\" 5 times\n\nfor(i in 1:5) {\n  print(\"Hello world\")\n}\n\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n[1] \"Hello world\"\n\n\nThe i in the for statements is the variable that will sequentially take all the values of the object (usually a vector) specified on the right hand side of the in keyword. In majority of the cases, the object is a sequence of integers, as in the example below, where i takes the values of each element of the vector 1:5 and prints it.\n\nfor(i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nA for loop could be used to add a constant to each element of a vector:\n\nx &lt;- c(4, 5, 1, 2, 9, 8, 0, 5, 3)\nx\n\n[1] 4 5 1 2 9 8 0 5 3\n\n#for all integers between 1 and length of vector x:\nfor(i in 1:length(x)) { \n  x[i] &lt;- x[i] + 5\n}\nx\n\n[1]  9 10  6  7 14 13  5 10  8\n\n\nHowever, in R this is redundant, because of vectorization (see the section on vectors from chapter 2). The above statement os equivalent to:\n\nx &lt;- c(4, 5, 1, 2, 9, 8, 0, 5, 3)\nx + 5\n\n[1]  9 10  6  7 14 13  5 10  8\n\n\nThis is not only simpler, but also more efficient.\nAnother, more practical aplication of the for loop could examine all columns of a data frame for missing values, so that:\n\ndev &lt;- read.csv(\"data/un_data/dev2018.csv\",\n                stringsAsFactors = FALSE)\nmissing &lt;- numeric() #create empty numeric vector\nfor (i in 1:length(dev)){\n  missing[i] &lt;- sum(is.na(dev[,i])) #get sum of missing for ith column\n  names(missing)[i] &lt;- names(dev)[i] #name it with ith column name\n}\nmissing\n\ncountry     eys     gni    lexp     mys \n      0       1       3       3       5 \n\n\nFrom this, we can see that there are 0 misisng values in the country name, 1 missing value in the expected years of schooling variable and, 3 missing values in gni and life expectancy and 5 missing values in mean years of schooling.\nWhile this is a bit more useful than the previous example, R still offers a shorthand method for such problems, which is discussed in more detail in the last part of this chapter. In general, due to the phenomena of vectorization, for loops are rarely used in simple data analysis in R. However, they are a core element of programming as such, therefore it’s important to understand them. In fact, vectorization is made possible only because of for loops being used by R in the background - simply their faster and more efficient versions.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Programming Concepts</span>"
    ]
  },
  {
    "objectID": "05-key_programming_concepts.html#if-statements",
    "href": "05-key_programming_concepts.html#if-statements",
    "title": "Key Programming Concepts",
    "section": "If statements",
    "text": "If statements\nIf statements are another crucial programming concept. They essentially allow performing computation conditionally on a logical statement. In other words, depending on a logical expression an operation is performed or not. If loops in R are constructed in the following way:\n\nif (logical_expression) {\n  operations\n}\n\nWhere logical_expression must an expression that evaluates to a logical value, for example X &gt; 5, country == \"France\" or is.na(x). operations are performed if and only if the logical_expression evaluates to TRUE. The simples possible example would be\n\nx &lt;- 2\nif (x &gt; 0) {\n  print(\"the value is greater than 0\")\n}\n\n[1] \"the value is greater than 0\"\n\nx &lt;- -2\nif (x &gt; 0) {\n  print(\"the value is greater than 0\")\n}\n\nIf is naturally complemented by the else clause, i.e. the operations that should be performed otherwise. The general form of such statement is:\n\nif (logical_expression) {\n  operations\n} else {\n  other_operations\n}\n\nIn this case, R first checks if the logical_expression evaluates to TRUE, and if it doesn’t, performs the other_operations. For example:\n\nx &lt;- -2\nif (x &gt; 0) {\n  print(\"the value is greater than 0\")\n} else {\n  print(\"the value is less or equal than 0\")\n}\n\n[1] \"the value is less or equal than 0\"\n\n\nFinally, else if allows to provide another statement to be evaluated. The general form of such statement would be:\n\nif (logical_statement) { \n  operation\n} else if (other_logical_statement) {\n  other_operation\n} else {\n  yet_another_operation\n}\n\nHere, R first checks the logical_statement, if it’s FALSE then it proceeds to check the other_logical_statement. If the second one is TRUE if performs the other_operation and if it’s FALSE it proceeds to perform the yet_another_operation. An extension of the previous example:\n\nx &lt;- 2\nif (x &gt; 0) {\n  print(\"The value is positive\")\n} else if (x &lt; 0) {\n  print(\"The value is negative\") \n} else {\n  print(\"The value is 0\")\n}\n\n[1] \"The value is positive\"\n\n\nIF-ELSE statments can be used to conditionally replace values. For example, suppose that we want to create a variable that is 1 when country is France and 0 otherwise. We could do that by:\n\ndev$france &lt;- 0\nfor (i in 1:nrow(dev)) {\n  if (dev$country[i] == \"France\") {\n    dev$france[i] &lt;- 1\n  }\n}\n\ndev$france[dev$country == \"France\"]\n\n[1] 1\n\n\nAgain, because of vectorization, R offers a shorthand for this, through the ifelse() function:\n\ndev$france &lt;- ifelse(dev$country == \"France\", 1, 0)\ndev$france[dev$country == \"France\"]\n\n[1] 1\n\n\nWhen you look at the documentation ?ifelse, you can see that it takes three arguments - test, yes and no. The test argument is the logical condition - same as logical_statement in the if, with the small subtle difference that it can evaluate to a logical vector rather than one single logical value. The yes argument is the value returned by the function if the test is TRUE and the no argument is returned when test is FALSE. You can fully see this in the example below:\n\nifelse(c(TRUE, FALSE, FALSE, TRUE), \"yes\", \"no\")\n\n[1] \"yes\" \"no\"  \"no\"  \"yes\"\n\nifelse(c(TRUE, FALSE, FALSE, TRUE), 1, 0)\n\n[1] 1 0 0 1",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Programming Concepts</span>"
    ]
  },
  {
    "objectID": "05-key_programming_concepts.html#functions",
    "href": "05-key_programming_concepts.html#functions",
    "title": "Key Programming Concepts",
    "section": "Functions",
    "text": "Functions\nR is known as a functional programming language - as you have already seen, almost all of the operations performed are done using functions. It is also possible to create our own, custom functions by combining other functions and data structures. This is done using the function() keyword. The general syntax of a function looks as follows:\n\nfunction_name &lt;- function(arg1, arg2) {\n  output &lt;- operations(arg1, arg2)\n  output\n}\n\nAs with any R object, you can use almost any name instead of function_name. Arguments are separeted by commas (in the above example arg1, arg2) - these are the objects you pass to your function on which you perform some arbitrary operations. Again, the arguments can have arbitrary names, but you need to use them within the function consistently. Finally, most of the functions return a value - this is the last object called within the function (output in the above example).\nAfter creating the function we can run it, exactly the same way as we would with any of R’s built-in functions. A simple example could return the number of missing values in an object:\n\ncount_na &lt;- function(x) {\n  sum(is.na(x))\n}\n\ncount_na(dev$mys)\n\n[1] 5\n\n\nWe could also implement our own summary statistics function, similar to describe() discussed in the previous chapter:\n\nsummary_stats &lt;- function(x) {\n  if (is.numeric(x)) {\n    list(Mean = mean(x, na.rm = TRUE), \n                SD = sd(x, na.rm = TRUE), \n                IQR = IQR(x, na.rm = TRUE))\n  } else if (is.character(x)) {\n    list(Length = length(x), \n                  Mean_Nchar = mean(nchar(x)))\n  } else if (is.factor(x)) {\n  list(Length = length(x), \n       Nlevels  = length(levels(x)))\n  }\n}\n\nLet’s walk through the above function Given a vector x, the function : 1. Checks whether x is a numeric vector. If so, returns a list of it’s mean, standard deviation and interquartile range. 2. Else, checks if x is a character vector. If so, returns a list containng its length and average number of characters. 3. Else, checks if x is a factor. If so returns a list containing its length and average number of character.\nWe can see how it works below:\n\nsummary_stats(c(1, 2, 3, 10))\n\n$Mean\n[1] 4\n\n$SD\n[1] 4.082483\n\n$IQR\n[1] 3\n\nsummary_stats(dev$country)\n\n$Length\n[1] 195\n\n$Mean_Nchar\n[1] 9.902564\n\nsummary_stats(as.factor(dev$country))\n\n$Length\n[1] 195\n\n$Nlevels\n[1] 195\n\n\n\nKeyword arguments\nMany of the functions used in R come with so-called default arguments - this was already mentioned in sorting. When defining our own functions, we can make use of that functionality as well. For example, the count_na example can be modified in the following way:\n\ncount_na &lt;- function(x, proportion = TRUE) {\n  num_na &lt;- sum(is.na(x))\n  if (proportion == TRUE) {\n    num_na/length(x)\n  } else {\n    num_na\n  }\n}\n\nThe proportion argument controls whether the function returns the number of NAs as value or as proportion of the entire vector:\n\ncount_na(dev$gni)\n\n[1] 0.01538462\n\ncount_na(dev$gni, proportion = TRUE) #same as above\n\n[1] 0.01538462\n\ncount_na(dev$gni, proportion = FALSE)\n\n[1] 3\n\n\nThere are couple of reasons why functions are frequently applied when analyzing data: 1. To avoid repetition - often, you need to perform the same operation repeatedly - sometimes on a dataframe with tens or hunderds of columns or even multiple data frames. To avoid re-writing the same code over and over again (which always increases the chance of an error occuring). 2. To enhance clarity - when you perform a long and complicated series of operations on a dataset, it’s often much easier to break it down into functions. Then when you need to come back to your code after a long time, it is often much easier to see recode_missing_values(data) appear in your code, with the record_missing_values function defined somewhere else, as you don’t need to go through your code step by step, but only understand what particular functions return. 3 To improve performance - while most of the operations we’ve seen in R take fractions of seconds, larger data can often lead to longer computation times. Functions can be combined with other tools to make computation more elegant and quicker - some of these methods are discussed in the next section.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Programming Concepts</span>"
    ]
  },
  {
    "objectID": "05-key_programming_concepts.html#sapply",
    "href": "05-key_programming_concepts.html#sapply",
    "title": "Key Programming Concepts",
    "section": "Sapply",
    "text": "Sapply\nRecall the code we used to check each column of our data frame for missingness in the for loops section:\n\nmissing &lt;- numeric() #create empty numeric vector\nfor (i in 1:length(dev)){\n  missing[i] &lt;- sum(is.na(dev[,i])) #get sum of missing for ith column\n  names(missing)[i] &lt;- names(dev)[i] #name it with ith column name\n}\n\nWe could re-write it using our new knowledge of functions, such that:\n\ncount_na &lt;- function(x) {\n  sum(is.na(x))\n}\n\nmissing &lt;- numeric()\nfor (i in 1:length(dev)) {\n  missing[i] &lt;- count_na(dev[,i])\n  names(missing)[i] &lt;- names(dev)[i]\n}\nmissing\n\ncountry     eys     gni    lexp     mys  france \n      0       1       3       3       5       0 \n\n\nWhile this may look a bit more fancy, in fact more code was used to perform this operation and it doesn’t differ too much in terms of clarity. The exact same result can be achieved using the sapply() function. sapply() takes two arguments - an R object, such as a vector and a data frame and a function. Then, it applies the function to each element of this object (i.e. value in case of vectors, column/variable in case of data frames).\n\nsapply(dev, count_na)\n\ncountry     eys     gni    lexp     mys  france \n      0       1       3       3       5       0 \n\n\nThe result is exactly the same as in the previous case. sapply() used the count_na function on each columns of the dev dataset.\nWhen using short, simple functions, sapply() can be even more concise, as we can defined our function without giving it a name. In the example below, instead of defining count_na separately, we define it directly within the sapply() call (i.e. inside the parentheses). This yields the same result.\n\nsapply(dev, function(x) sum(is.na(x)))\n\ncountry     eys     gni    lexp     mys  france \n      0       1       3       3       5       0 \n\n\nConsider the function below. What do you expect it to return? Try going through each element of the code separately. You can check how the rowSums command works by typing ?rowSums into the R console.\n\nquartile &lt;- function(x) {\n  quantiles &lt;- quantile(x, c(0.25, 0.5, 0.75), na.rm = TRUE)\n  comparisons &lt;- sapply(quantiles, function(y) y &lt;= x)\n  rowSums(comparisons) + 1\n}\n\nThe function takes a vector as input and computes three quantiles of its values - 25%, 50%, 75%. You may recall from the previous chapter that quantiles are cut points that divide a variable into ranges of equal proportions in the data set. The resulting quantiles vector consists of three values, corresponding with thre three quantiles. We then use sapply on these three values to compare each of them with the value of the x vector. As a result, we obtain a 3 x n array, where n is length of x. For each of the values of x we get three logical values. Each of them is TRUE when the corresponding value of x was larger than the quantile and FALSE if the corresponding value of x was lower than the quantile. We can then sum the results by row, using rowSums. Our final result is a vector with values of 0, 1 and 2. Its value is 0 if the corresponding value of x was less than all quartiles, 1 if it was greater or equal than the .25, 2 if it was greater or equal than 0.5 and 3 if it was greater or equal than all of them. We then finally add 1 to each, so that they correspond to true quartile numbers (1st quartile, rather than 0th quartile, etc).\nWe can then use the split function, which takes a data frame and a vector as input and splits the data frame into several parts, each with the same value of the splitting variable. As a result, we obtain dev_split dataset, which stores 4 data frames, each only with countries in the respective quantile of expected years of schooling.\n\ndev_split &lt;- split(dev, quartile(dev$eys))\nhead(dev_split[[1]])\n\n                    country  eys  gni lexp mys france\n1               Afghanistan 10.1 1746 64.5 3.9      0\n14               Bangladesh 11.2 4057 72.3 6.1      0\n27             Burkina Faso  8.9 1705 61.2 1.6      0\n33 Central African Republic  7.6  777 52.8 4.3      0\n34                     Chad  7.5 1716 54.0 2.4      0\n38                  Comoros 11.2 2426 64.1 4.9      0\n\n\nYou can then look at descriptive statistics of each of the quartiles using:\n\nsapply(dev_split, summary)\n\n$`1`\n   country               eys              gni             lexp      \n Length:47          Min.   : 5.000   Min.   :  777   Min.   :52.80  \n Class :character   1st Qu.: 8.700   1st Qu.: 1611   1st Qu.:60.80  \n Mode  :character   Median : 9.700   Median : 2318   Median :64.30  \n                    Mean   : 9.415   Mean   : 3579   Mean   :63.89  \n                    3rd Qu.:10.550   3rd Qu.: 3731   3rd Qu.:67.00  \n                    Max.   :11.200   Max.   :17796   Max.   :75.10  \n                                     NA's   :1                      \n      mys            france \n Min.   :1.600   Min.   :0  \n 1st Qu.:3.700   1st Qu.:0  \n Median :4.850   Median :0  \n Mean   :4.861   Mean   :0  \n 3rd Qu.:6.075   3rd Qu.:0  \n Max.   :9.800   Max.   :0  \n NA's   :1                  \n\n$`2`\n   country               eys             gni              lexp      \n Length:50          Min.   :11.30   Min.   :   660   Min.   :58.90  \n Class :character   1st Qu.:11.80   1st Qu.:  4232   1st Qu.:68.03  \n Mode  :character   Median :12.30   Median :  6903   Median :71.50  \n                    Mean   :12.22   Mean   : 10788   Mean   :70.39  \n                    3rd Qu.:12.70   3rd Qu.: 11578   3rd Qu.:73.83  \n                    Max.   :13.00   Max.   :110489   Max.   :80.10  \n                                                     NA's   :2      \n      mys             france \n Min.   : 3.100   Min.   :0  \n 1st Qu.: 6.500   1st Qu.:0  \n Median : 7.850   Median :0  \n Mean   : 7.869   Mean   :0  \n 3rd Qu.: 9.475   3rd Qu.:0  \n Max.   :11.600   Max.   :0  \n NA's   :2                   \n\n$`3`\n   country               eys             gni             lexp      \n Length:47          Min.   :13.10   Min.   : 3317   Min.   :63.90  \n Class :character   1st Qu.:13.65   1st Qu.:10694   1st Qu.:74.53  \n Mode  :character   Median :14.30   Median :14356   Median :76.05  \n                    Mean   :14.19   Mean   :22644   Mean   :75.45  \n                    3rd Qu.:14.70   3rd Qu.:26054   3rd Qu.:76.88  \n                    Max.   :15.10   Max.   :99732   Max.   :82.10  \n                                    NA's   :1       NA's   :1      \n      mys             france \n Min.   : 5.500   Min.   :0  \n 1st Qu.: 8.600   1st Qu.:0  \n Median : 9.900   Median :0  \n Mean   : 9.883   Mean   :0  \n 3rd Qu.:11.200   3rd Qu.:0  \n Max.   :12.600   Max.   :0  \n NA's   :1                   \n\n$`4`\n   country               eys             gni             lexp      \n Length:50          Min.   :15.20   Min.   : 9570   Min.   :72.40  \n Class :character   1st Qu.:15.68   1st Qu.:24906   1st Qu.:77.25  \n Mode  :character   Median :16.35   Median :34918   Median :81.20  \n                    Mean   :16.79   Mean   :35322   Mean   :79.66  \n                    3rd Qu.:17.40   3rd Qu.:45698   3rd Qu.:82.38  \n                    Max.   :22.10   Max.   :83793   Max.   :84.70  \n      mys            france    \n Min.   : 7.70   Min.   :0.00  \n 1st Qu.:10.43   1st Qu.:0.00  \n Median :12.25   Median :0.00  \n Mean   :11.55   Mean   :0.02  \n 3rd Qu.:12.70   3rd Qu.:0.00  \n Max.   :14.10   Max.   :1.00  \n\n\nWhile working an R and looking for help online, you may stumble upon other variants of the sapply() functions. Essentially, all R functions with apply in their name serve the same purpose - applying a function to each element of an object. lapply() is a less user friendy version of sapply(), which always returns a list, not a vector. vapply() forces the user to determine the type of the output, which makes its behaviour more predictible and slightly faster. tapply() applies the function to data frame by group determined by another variable - a similar procedure to what we did using split() and sapply(), but in less steps.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Key Programming Concepts</span>"
    ]
  },
  {
    "objectID": "06-dplyr_data_manipulation.html",
    "href": "06-dplyr_data_manipulation.html",
    "title": "Data Manipulation",
    "section": "",
    "text": "Introduction\nWhile we have already discussed some methods for data manipulation, such as indexing, subsetting and modifying data frames, the majority of R users approach this task using a dedicated collection of packages called tidyverse, introduced by Hadley Wickham, a statistician from New Zealand. While it may seem like this chapter covers tools for performing task that you are already familiar with, tidyverse follows a different philosophy than traditional R, and has a lot of advantages including better code readability and efficiency. As before, to install and load the package, simply run the code below. Remember, that you only need to call install.packages once, to download all the required files from CRAN. library needs to be called every time you start a new R session to attach the library functions to your working environment.\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-dplyr_data_manipulation.html#the-pipe-operator",
    "href": "06-dplyr_data_manipulation.html#the-pipe-operator",
    "title": "Data Manipulation",
    "section": "The pipe operator",
    "text": "The pipe operator\nPerhaps the most imporant innovation offered by the tidyverse package is the so-called pipe operator %&gt;%. Its use may feel a bit quirky at first, but it is extremely useful and is widely used by most of modern R users.\nAs you have learned so-far, R evaluates each function from inside out. For example, we can get the sum of missing values in a data frame by running:\n\nsum(is.na(math))\n\n[1] 10\n\n\nThis essentially performs two steps - first, runs is.na on the math data frame, which returns a table filled with logical values, FALSE when a given entry is not missing and TRUE when it is. Then sum takes this table as input and adds up the values in it (treating FALSE as 0 and TRUE as 1). In many cases, such statements can get long, difficult to read and error-prone, especially when keyword arguments are specified.\nThe same operation may be be done using the pipe operator. In this case, rather than evaluating the sequence of functions from within, they are evaluated left to right. The %&gt;% operator can be understood as a way of _passing the output of the thing on the left to the thing on the right as the first argument:\n\nmath %&gt;% is.na() %&gt;% sum()\n\n[1] 10\n\n\nIn this example, the math data frame is passed to the is.na function, and then the output is passed to the sum function, which returns exactly the same result. As in the case of the regular call, you may store the output in a variable:\n\nnumber_missing &lt;- math %&gt;% is.na() %&gt;% sum()\nnumber_missing\n\n[1] 10\n\n\nBefore continuing we drop the missing observations:\n\nmath &lt;- math[complete.cases(math), ]\n\nWhile this may feel slightly unintuitive in this case, it comes in very handy when performing long sequences of operations on data, as we will see in the following sections.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-dplyr_data_manipulation.html#reshaping-data",
    "href": "06-dplyr_data_manipulation.html#reshaping-data",
    "title": "Data Manipulation",
    "section": "Reshaping data",
    "text": "Reshaping data\nIt’s not common for social scientific to be longitudinal in nature. This means, that data in a given unit of observation (for example country, household or an individual) is observed on multiple variable (for example GDP, income, well-being) over a period of time. Such data can come in two formats - long and wide.\nWide data format - in the wide data format, each column represents a variable - for example, the table presented below presents grades of three students over three academic years in the wide format. Each column represents a separate year.\n\n\n\n\n\nstudent\n1\n2\n3\n\n\n\n\nThomas\n2.34\n3.87\n2.03\n\n\nMary\n3.87\n4.58\n2.70\n\n\nDavid\n3.83\n3.92\n4.00\n\n\n\n\n\nLong data format - in the long data format, a separate column represents the name of the variable and a separate one - value of the corresponding variable. This format is sometimes more useful for particular types of analysis such as panel data models and for visualization. You can see the student scores in the long format below:\n\n\n\n\n\nstudent\nyear\ngrade\n\n\n\n\nThomas\n1\n2.34\n\n\nThomas\n2\n3.87\n\n\nThomas\n3\n2.03\n\n\nMary\n1\n3.87\n\n\nMary\n2\n4.58\n\n\nMary\n3\n2.70\n\n\nDavid\n1\n3.83\n\n\nDavid\n2\n3.92\n\n\nDavid\n3\n4.00\n\n\n\n\n\nWe can see an example of such data by loading the dataset gdp.csv, which contains GDP per capita over several years for couple of European countries:\n\ngdp &lt;- read_csv(\"data/world_bank/gdp.csv\")\n\n\nhead(gdp)\n\n# A tibble: 6 × 23\n  country   ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany   DEU   27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250. 37802.\n2 Denmark   DNK   28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966. 41278.\n3 Spain     ESP   21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436. 33263.\n4 France    FRA   26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086. 35095.\n5 United K… GBR   26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597. 36660.\n6 Greece    GRC   19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290. 30856.\n# ℹ 12 more variables: `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;,\n#   `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, X65 &lt;lgl&gt;\n\n\nWe can see that the data contains country names, as well as GDP values in years between 2000 and 2015 in the wide format. To reshape the data into the long format, we can use the pivot_longer function, which comes with the tidyr package, another element of the tidyverse suite. In pivot longer, we specify the dataset name as the first argument (which is usually piped to the function), followed by the column names that contain the wide-format variables (assuming that they are in order, this can be specified with a names of the left-most and right-most variable, separated by a colon). Note that in this example, we also use the inverse quotation marks, since the variable are named using numbers. The names_to argument specifies tha name of the variable which will be used to store the names of the re-formatted variables (in our example - years) and the value_to argument specifies the name of the variable which will be used to store values (GDP per capita).\n\ngdp_long &lt;- gdp %&gt;% pivot_longer(`2000`:`2019`, \n                                 names_to = \"year\", values_to = \"gdp_pc\")\nhead(gdp_long)\n\n# A tibble: 6 × 5\n  country ccode X65   year  gdp_pc\n  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt;\n1 Germany DEU   NA    2000  27209.\n2 Germany DEU   NA    2001  28381.\n3 Germany DEU   NA    2002  29179.\n4 Germany DEU   NA    2003  29875.\n5 Germany DEU   NA    2004  31305.\n6 Germany DEU   NA    2005  31794.\n\n\nAs you can see, the function produces data in a long format, with only 4 columns, but 140 rows, as opposed to the wide data which consists of only 7 rows, but 22 columns.\nIn some cases, your data might come in a long format, yet you might want to reshape it into long. This can be done using the pivot_wider function. This works exactly opposite to pivot_longer. We first specify the data by piping it to the function and then use the names_from argument to specify the name of the variable containing the variable names and value_from to specify the variable containing the values. We end up obtaining the same data frame that we started with.\n\ngdp_wide &lt;- gdp_long %&gt;% pivot_wider(names_from = \"year\", values_from = \"gdp_pc\")\nhead(gdp_wide)\n\n# A tibble: 6 × 23\n  country    ccode X65   `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007`\n  &lt;chr&gt;      &lt;chr&gt; &lt;lgl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany    DEU   NA    27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250.\n2 Denmark    DNK   NA    28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966.\n3 Spain      ESP   NA    21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436.\n4 France     FRA   NA    26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086.\n5 United Ki… GBR   NA    26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597.\n6 Greece     GRC   NA    19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290.\n# ℹ 12 more variables: `2008` &lt;dbl&gt;, `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;,\n#   `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;,\n#   `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;\n\n\n\nall.equal(gdp_wide, gdp)\n\n [1] \"Names: 21 string mismatches\"                                                           \n [2] \"Attributes: &lt; Names: 1 string mismatch &gt;\"                                              \n [3] \"Attributes: &lt; Length mismatch: comparison on first 2 components &gt;\"                     \n [4] \"Attributes: &lt; Component \\\"class\\\": Lengths (3, 4) differ (string compare on first 3) &gt;\"\n [5] \"Attributes: &lt; Component \\\"class\\\": 3 string mismatches &gt;\"                              \n [6] \"Attributes: &lt; Component 2: Modes: numeric, externalptr &gt;\"                              \n [7] \"Attributes: &lt; Component 2: Lengths: 7, 1 &gt;\"                                            \n [8] \"Attributes: &lt; Component 2: target is numeric, current is externalptr &gt;\"                \n [9] \"Component 3: Modes: logical, numeric\"                                                  \n[10] \"Component 3: target is logical, current is numeric\"                                    \n[11] \"Component 4: Mean relative difference: 0.04964384\"                                     \n[12] \"Component 5: Mean relative difference: 0.04797772\"                                     \n[13] \"Component 6: Mean relative difference: 0.02722602\"                                     \n[14] \"Component 7: Mean relative difference: 0.05492863\"                                     \n[15] \"Component 8: Mean relative difference: 0.03197982\"                                     \n[16] \"Component 9: Mean relative difference: 0.08535064\"                                     \n[17] \"Component 10: Mean relative difference: 0.04919092\"                                    \n[18] \"Component 11: Mean relative difference: 0.04411082\"                                    \n[19] \"Component 12: Mean relative difference: 0.02757141\"                                    \n[20] \"Component 13: Mean relative difference: 0.05151733\"                                    \n[21] \"Component 14: Mean relative difference: 0.04769779\"                                    \n[22] \"Component 15: Mean relative difference: 0.01895478\"                                    \n[23] \"Component 16: Mean relative difference: 0.03867191\"                                    \n[24] \"Component 17: Mean relative difference: 0.03078083\"                                    \n[25] \"Component 18: Mean relative difference: 0.02475497\"                                    \n[26] \"Component 19: Mean relative difference: 0.05288349\"                                    \n[27] \"Component 20: Mean relative difference: 0.05189977\"                                    \n[28] \"Component 21: Mean relative difference: 0.03430557\"                                    \n[29] \"Component 22: Mean relative difference: 0.04530826\"                                    \n[30] \"Component 23: Modes: numeric, logical\"                                                 \n[31] \"Component 23: target is numeric, current is logical\"",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-dplyr_data_manipulation.html#joining",
    "href": "06-dplyr_data_manipulation.html#joining",
    "title": "Data Manipulation",
    "section": "Joining",
    "text": "Joining\nThe final data manipulation technique that we will discuss in this chapter is joining. In many cases we will have the dataset coming in two or more separate file, each containing different variables for the same unit for observations. This is the case with all the data coming from World Bank Open Data, where information about each indicator comes in a separate csv file. For example, suppose we have data on GDP and population density of some countries:\n\ngdp &lt;- read_csv(\"data/world_bank/gdp.csv\")\npop &lt;- read_csv(\"data/world_bank/pop_dens.csv\")\n\n\nhead(gdp)\n\n# A tibble: 6 × 23\n  country   ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany   DEU   27209. 28381. 29179. 29875. 31305. 31794. 34119. 36250. 37802.\n2 Denmark   DNK   28669. 29450. 30640. 30787. 32909. 34150. 37289. 38966. 41278.\n3 Spain     ESP   21592. 22959. 24372. 25019. 26120. 27607. 30683. 32436. 33263.\n4 France    FRA   26100. 27502. 28524. 28142. 29034. 30499. 32429. 34086. 35095.\n5 United K… GBR   26413. 27757. 29069. 30262. 31965. 32668. 34761. 35597. 36660.\n6 Greece    GRC   19524. 20964. 22616. 23871. 25437. 25578. 28515. 29290. 30856.\n# ℹ 12 more variables: `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;,\n#   `2018` &lt;dbl&gt;, `2019` &lt;dbl&gt;, X65 &lt;lgl&gt;\n\n\n\nhead(pop)\n\n# A tibble: 6 × 23\n  country   ccode `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007` `2008`\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 France    FRA    111.   112.   113.   114.   115.   115.   116.   117.   118. \n2 United K… GBR    243.   244.   245.   247.   248.   250.   252.   253.   255. \n3 Ireland   IRL     55.2   56.1   57.1   58.0   59.1   60.4   62.0   63.9   65.2\n4 Italy     ITA    194.   194.   194.   195.   196.   197.   198.   199.   200. \n5 Poland    POL    125.   125.   125.   125.   125.   125.   125.   124.   124. \n6 Portugal  PRT    112.   113.   114.   114.   115.   115.   115.   115.   115. \n# ℹ 12 more variables: `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;,\n#   `2013` &lt;dbl&gt;, `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;,\n#   `2018` &lt;dbl&gt;, `2019` &lt;lgl&gt;, X65 &lt;lgl&gt;\n\n\nSuppose we want to analyze the relationship between population density and gdp per capita. To do that, it would be convenient to merge these two datasets into one, containing the variables gdp and pop_dens. We can achieve this by using joins.\nFirst, we pivot the data into the long format:\n\ngdp_long &lt;- gdp %&gt;% pivot_longer(`2000`:`2019`, \n                                 names_to = \"year\", values_to = \"gdp_pc\")\npop_long &lt;- pop %&gt;% pivot_longer(`2000`:`2019`, \n                                 names_to = \"year\", values_to = \"pop_dens\")\n\nNote that the countries in the two datasets are different:\n\nunique(gdp_long$country)\n\n[1] \"Germany\"        \"Denmark\"        \"Spain\"          \"France\"        \n[5] \"United Kingdom\" \"Greece\"         \"Poland\"        \n\n\n\nunique(pop_long$country)\n\n[1] \"France\"         \"United Kingdom\" \"Ireland\"        \"Italy\"         \n[5] \"Poland\"         \"Portugal\"      \n\n\n\nidentical(unique(gdp_long$country), unique(pop_long$country))\n\n[1] FALSE\n\n\nTo join two data frames, we need an ID variable (or a set of variables) that will identify observations and allow us to join them. In the example, the country and year variables are a perfect candidate, since each corresponds to one observation from a given country in a given period. We would then say that we join the two datasets on country and year. s\nThere are three fundamental ways in which we can approach this:\n\nInner join is used to join only the observations where the variables we are joining on which appear in both datasets. The rows where the identifying variables don’t match any observations in the other dataset are dropped from the resulting dataset. This join is used when we care primarily about the completeness of our data. The order of the dataframes does not matter when performing inner join.\n\n\ndat &lt;- inner_join(gdp_long, pop_long, by = c(\"country\", \"ccode\", \"year\"))\nhead(dat)\n\n# A tibble: 6 × 7\n  country ccode X65.x year  gdp_pc X65.y pop_dens\n  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;\n1 France  FRA   NA    2000  26100. NA        111.\n2 France  FRA   NA    2001  27502. NA        112.\n3 France  FRA   NA    2002  28524. NA        113.\n4 France  FRA   NA    2003  28142. NA        114.\n5 France  FRA   NA    2004  29034. NA        115.\n6 France  FRA   NA    2005  30499. NA        115.\n\n\nWe can see that the new dataframe dat contains both gdp_pc and pop_dens variables. Furthermore, only the countries present in both datasets were kept:\n\nunique(dat$country)\n\n[1] \"France\"         \"United Kingdom\" \"Poland\"        \n\n\n\nLeft join is used to join only the observations where the variables we are joining appear in the first dataset (the one on the left of the joining function). This is done primarily when we care about keeping all the observations from the first (left) dataset. The observations where no corresponding identifying values were found are turned into missing values:\n\n\ndat &lt;- left_join(gdp_long, pop_long, by = c(\"country\", \"ccode\", \"year\"))\nhead(dat)\n\n# A tibble: 6 × 7\n  country ccode X65.x year  gdp_pc X65.y pop_dens\n  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;\n1 Germany DEU   NA    2000  27209. NA          NA\n2 Germany DEU   NA    2001  28381. NA          NA\n3 Germany DEU   NA    2002  29179. NA          NA\n4 Germany DEU   NA    2003  29875. NA          NA\n5 Germany DEU   NA    2004  31305. NA          NA\n6 Germany DEU   NA    2005  31794. NA          NA\n\n\nAs we can see in this example, the resulting dataset contains missing values for the countries that were not present in the pop_long dataset. All the countries from the gdp_long dataset were kept:\n\nall.equal(unique(gdp_long$country), unique(dat$country))\n\n[1] TRUE\n\n\n\nFull join - joining observations from both data frames and producing missing values whenever there’s an observation missing in one of them.\n\n\ndat &lt;- full_join(gdp_long, pop_long, by = c(\"country\", \"ccode\", \"year\"))\nhead(dat)\n\n# A tibble: 6 × 7\n  country ccode X65.x year  gdp_pc X65.y pop_dens\n  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;\n1 Germany DEU   NA    2000  27209. NA          NA\n2 Germany DEU   NA    2001  28381. NA          NA\n3 Germany DEU   NA    2002  29179. NA          NA\n4 Germany DEU   NA    2003  29875. NA          NA\n5 Germany DEU   NA    2004  31305. NA          NA\n6 Germany DEU   NA    2005  31794. NA          NA\n\n\nAs a result of a full_join, all countries including in either of the datasets are kept:\n\nunique(c(dat$country))\n\n [1] \"Germany\"        \"Denmark\"        \"Spain\"          \"France\"        \n [5] \"United Kingdom\" \"Greece\"         \"Poland\"         \"Ireland\"       \n [9] \"Italy\"          \"Portugal\"      \n\nunique(c(gdp_long$country, pop_long$country))\n\n [1] \"Germany\"        \"Denmark\"        \"Spain\"          \"France\"        \n [5] \"United Kingdom\" \"Greece\"         \"Poland\"         \"Ireland\"       \n [9] \"Italy\"          \"Portugal\"      \n\n\nThere are some other joining techniques, such as Filtering joins (semi_join and anti_join), as well as nest_join. You can read more about these in the documentation by typing ?join into the console.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-dplyr_data_manipulation.html#aggregating-data",
    "href": "06-dplyr_data_manipulation.html#aggregating-data",
    "title": "Data Manipulation",
    "section": "Aggregating data",
    "text": "Aggregating data\nWhile we have discussed summary statistics that can be used to summarize data, it’s often very useful to compare their values across group, rather than only look at one number to describe an entire dataset. The tidyverse allows us to calculate summary statistics of variables through the summarise function. For example, to get the average GDP of countries in our data:\n\ngdp_long %&gt;% summarise(gdp_avg = mean(gdp_pc))\n\n# A tibble: 1 × 1\n  gdp_avg\n    &lt;dbl&gt;\n1  33486.\n\n\nThis is no different from using gdp_long$gdp_pc %&gt;% mean(), other than it returns a tibble rather than a scalar value. However, the summarize function is most powerful in conjunction with the group_by function. As the name suggests, group_by function divides the data frame into groups using one of the variables. On the surface, it doesn’t appear to alter much:\n\ngdp_long_groupped &lt;- gdp_long %&gt;% group_by(country)\ngdp_long_groupped\n\n# A tibble: 140 × 5\n# Groups:   country [7]\n   country ccode X65   year  gdp_pc\n   &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1 Germany DEU   NA    2000  27209.\n 2 Germany DEU   NA    2001  28381.\n 3 Germany DEU   NA    2002  29179.\n 4 Germany DEU   NA    2003  29875.\n 5 Germany DEU   NA    2004  31305.\n 6 Germany DEU   NA    2005  31794.\n 7 Germany DEU   NA    2006  34119.\n 8 Germany DEU   NA    2007  36250.\n 9 Germany DEU   NA    2008  37802.\n10 Germany DEU   NA    2009  36851.\n# ℹ 130 more rows\n\n\n\npop_long %&gt;%\n  group_by(country) %&gt;%\n  summarise(avg_pop = mean(pop_dens, na.rm = TRUE), \n            sd_pop = sd(pop_dens, na.rm = TRUE))\n\n# A tibble: 6 × 3\n  country        avg_pop sd_pop\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 France           118.   3.60 \n2 Ireland           63.9  4.89 \n3 Italy            200.   4.67 \n4 Poland           124.   0.301\n5 Portugal         114.   1.13 \n6 United Kingdom   258.  10.3  \n\n\nSimilarily, we could compare the country’s average, maximum and minimum GDP growth over the past years:\n\ngdp_long %&gt;% \n  group_by(country) %&gt;%\n  mutate(gdp_growth = (gdp_pc - lag(gdp_pc)) / lag(gdp_pc)) %&gt;%\n  summarise(avg_growth = mean(gdp_growth, na.rm = TRUE), \n            max_growth = max(gdp_growth, na.rm = TRUE), \n            min_growth = min(gdp_growth, na.rm = TRUE)) %&gt;%\n  arrange(-avg_growth)\n\n# A tibble: 7 × 4\n  country        avg_growth max_growth min_growth\n  &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Poland             0.0636     0.109      0.0361\n2 Denmark            0.0398     0.0919    -0.0220\n3 Germany            0.0391     0.0914    -0.0252\n4 Spain              0.0364     0.111     -0.0343\n5 France             0.0344     0.0633    -0.0134\n6 United Kingdom     0.0330     0.0641    -0.0445\n7 Greece             0.0264     0.115     -0.0730\n\n\nIn this case, we first group the data frame by country. We then use mutate to compute gdp_growth by subtracting GDP from one year before (calculated using the lag function) and dividing this difference by the lagged GDP. Note that the mutate function also applies the computation according to the grouping defined in the group_by - the lag() is computed within each country. We then use summarise to apply the functions. Finally, we use arrange to sort the output according to the negative value of the average GDP growth, i.e. in decreasing order.\nSince the behaviour of group_by affects many operations performed on a dataframe, it is important to call ungroup() at the end of our operations if we assign the data frame to a new name - performing operations on groupped tibbles can lead to suprising results. Coming back to our example, suppose we wanted to obtain the deviation of each country’s GDP growth from the global mean of this growth. First, we would obtain the growths:\n\ngdp_growths &lt;- gdp_long %&gt;% \n                group_by(country) %&gt;%\n                mutate(gdp_growth = (gdp_pc - lag(gdp_pc)) / lag(gdp_pc))\n\n#store demeaned values:\ndem &lt;- gdp_growths %&gt;% \n  mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE))\n\n#stored demeaned values calculated after ungrouping\ndem_ungr &lt;- gdp_growths %&gt;% \n  ungroup() %&gt;%\n  mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE))\n\nall.equal(dem$growth_dem, dem_ungr$growth_dem)\n\n[1] \"Mean relative difference: 0.3395083\"\n\n\nWe can see using the all.equal function, that the resulting variables are different. This is because, if the tibble is still groupped, the mutate(growth_dem = gdp_growth - mean(gdp_growth, na.rm = TRUE)) expression subtracts the group mean from each observation, wheres if it’s ungrouped, it calculates the global mean and subtracts it. While it may seem trivial in this example, forgetting to ungroup a tibble is a common error and it is crucial to remeber to ungroup the tibble after finishing performing operations on it. Also note that calling group_by() on an already groupped tibble discards the previous groupping and applies the new one instead.",
    "crumbs": [
      "ADVANCED DATA MANIPULATION",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html",
    "href": "07-data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Introduction\nThe final chapter in this part focuses on data visualization. While some techniques for presenting data were already described in the chapter on Exploratory data analysis, this chapter will cover the topic in more depth and introduce you to the most commonly used tool for data visualization in R - the ggplot2 package. The main advantage of ggplot2 in comparison with some of the out-of-the-box plotting capabilities coming with R is the simplicity of creating relatively complex and nice looking plot, which would often take much longer to produce with R base graphics.\nThe ggplot2 package is a part of tidyverse, which was introduced in the previous chapter, and it is often useful in conjunction with some of the data manipulation tools offered by other tidyverse packages. You can load the library directly:\nlibrary(ggplot2)\nHowever you may also attach it along with other tidyverse packages:\nlibrary(tidyverse)\nWe will also load the salaries dataset, which describes the salaries of US professors. To do this, you will need to install the package using the carData package using the install.packages function:\ninstall.packages('carData')\nTo load the data, simpy execute the below code. The dataframe will be attached to your global environment.\ndata('Salaries', package = 'carData')\nglimpse(Salaries)\n\nRows: 397\nColumns: 6\n$ rank          &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof,…\n$ discipline    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A,…\n$ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 1…\n$ yrs.service   &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3,…\n$ sex           &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Fe…\n$ salary        &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 14…\nThe data consists of 6 variables, describing academics rank, discipline (A for theoretical and B for applied), experience measured in years in service and years since PhD attainment, the subjects sex and salary. To see more details about the data, use ?carData::Salaries",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#density-estimate",
    "href": "07-data_visualization.html#density-estimate",
    "title": "Data Visualization",
    "section": "Density estimate",
    "text": "Density estimate\nThe density estimate is an alternative way to draw a histogram of the data. It represents the distribution of a continous variable by drawing a line. You can see that the shape of the density plot of absences below directly corresponds with the histogram of this variable from the previous example.\n\nggplot(Salaries, aes(x = salary)) + \n  geom_density() \n\n\n\n\n\n\n\n\nKernel density estimates are particularly useful to compare the distribution of a variable between different values of a grouping variable. For example, we can examine the difference in the distributions of the salary depending on the position held by the individual:\n\nggplot(Salaries, aes(x = salary, fill = rank)) + \n  geom_density(alpha = 0.3)\n\n\n\n\n\n\n\n\nThe differences are clear-cut - it appears that as expected, higher position held generally leads to higher median of earnings. However, each position also has higher variability in income, with the spread of Professors earnings more than twice as large as for Assistant Professors.",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#barplots",
    "href": "07-data_visualization.html#barplots",
    "title": "Data Visualization",
    "section": "Barplots",
    "text": "Barplots\nHistograms and kernel density estimates are useful in representing continuous variables, such as temperature, GDP or hours of absence. In case of discrete variables, which only take a finite and unordered set of values, it’s often better to use bar plots. With ggplot2, these can be easily obtained using geom_bar. For example, we can use it to represent the of academic ranks within our dataset.\n\nggplot(Salaries, aes(x = rank)) + \n  geom_bar(width = 0.5, fill = 'forestgreen')\n\n\n\n\n\n\n\n\nYou may also want to make the bar plots horizontal. There are two ways to to this: the old approach is simply to add coord_flip() to your ggplot object. You may still encounter this way of plotting horizontal bar plots in other users code.\n\nggplot(Salaries, aes(x = rank)) + \n  geom_bar(width = 0.5, fill = 'forestgreen') +\n  coord_flip()\n\n\n\n\n\n\n\n\nHowever, since ggplot version 3.3.0, providing the y axis instead of x axis leads to the same result. Furthermore, it doesn’t cause unnecessary confusion caused by flipping the axis (for example, the y-axis label needs to be specified as the x-axis label when coord_flip() is applied). Thus it is recommended to use the second approach, provided that you have the most up-to-date version of tidyverse installed.\n\nggplot(Salaries, aes(y = rank)) + \n  geom_bar(width = 0.5, fill = 'forestgreen')\n\nAgain, we can also fill the bars with another variable, simply by specifying the fill argument in the aesthetic mapping definition:\n\nggplot(Salaries, aes(x = rank, fill = sex)) + \n  geom_bar(width = 0.5)\n\n\n\n\n\n\n\n\nThe bars can also be placed side-by-side to improve comparison. To do that, we simply specify a named argument position to the geom_bar function and set it to 'dodge', as seen in the example below. This means, that one bar should ‘dodge’ another, and be placed next to it. The default value of the position argument is 'stacked', which is pretty self-explanatory and can be seen in the previous example.\n\nggplot(Salaries, aes(x = rank, fill = discipline)) + \n  geom_bar(position = 'dodge')\n\n\n\n\n\n\n\n\nWe can also use barplots to present summary statistics visually - for example, the average salary of a professor given his or her rank. To do that, we first construct a summary data frame, using tools covered in the previous chapter, and then plot the result.\n\nsalary_by_rank &lt;- Salaries %&gt;% \n  group_by(rank) %&gt;%\n  summarise(avg_salary = mean(salary)) %&gt;%\n  ungroup()\n\nggplot(salary_by_rank, aes(x = rank, y = avg_salary)) + \n  geom_bar(stat = 'identity', width = 0.5, fill = 'firebrick')\n\n\n\n\n\n\n\n\nNote that in this case we need to specify both x and y aesthetics, as x represents the name of the group (in this case the professors’ rank) and y - the associated value, indicated by the height of each bar. Moreover, we need to setthe stat argument of geom_bar to identity. This is because the default setting, stat = count implies that ggplot takes only one, discrete variable as arguments and counts the number of observations in each of the group defined by the levels of the variable - an operation very similar to the output of table(math$health). We can also use geom_col, which is a short-hand for geom_bar(stat = 'identity') and produces the same output:\n\nggplot(salary_by_rank, aes(x = rank, y = avg_salary)) + \n  geom_col(width = 0.5, fill = 'firebrick')",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#boxplots",
    "href": "07-data_visualization.html#boxplots",
    "title": "Data Visualization",
    "section": "Boxplots",
    "text": "Boxplots\nAnother way to compare a variable across groups is through the use of boxplots. Boxplots were described in the chapter on exploratory data analysis as a way of evaluating the distribution of a continuous variable. They are also implemented in the ggplot2 package and can be used for comparisons of the continuous variables distributions across multiple groups defined by a discrete variable. This can be done by specifying the grouping variable to the x aesthetic and the continuous variable which we want to examine to the y aesthetic.\n\nggplot(Salaries, aes(x = sex, y = salary)) + \n  geom_boxplot(width = 0.5)\n\n\n\n\n\n\n\n\nWe can see that while the medians of Male and Female professor’s wages in our sample are similar, the interquantile range is much higher for men. At the same time, the distribution of male wages is more spread out, as indicated by larger box, longer tails. It also includes some outliers.\nWe can also draw an arbitrary summary statistic - such as the mean - on the plot, by using the stat_summary function - we specify the statistic we want to use with the fun keyword argument and the geom using the geom argument. In this case, we add a cross to indicate the mean salary for both sexes on the plot:\n\nggplot(Salaries, aes(x = sex, y = salary)) + \n  geom_boxplot(width = 0.5) + \n  stat_summary(fun = \"mean\", geom = \"point\", \n               shape = 3, color = \"red\")",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#violin-plots",
    "href": "07-data_visualization.html#violin-plots",
    "title": "Data Visualization",
    "section": "Violin plots",
    "text": "Violin plots\nViolin plots are closely related to box plots, however they depict the shape of the variables’ distribution. As such, they overcome the issue related to the use of boxplots decribed in the exploratory analysis chapter - namely that two distributions can have identical box plots, yet very different underlying shapes. The violin plots avoid it by connecting boxplots with kernel density estimates, giving a better approximation of the shape of a variable’s distribution.\n\nggplot(Salaries, aes(x = sex, y = salary)) + \n  geom_violin(width = 0.5)\n\n\n\n\n\n\n\n\nWe can see the strength of that by coming back to the misleading example from the previous chapter. Suppose we have the\n\n\n\n\n\n\n\n\n\nThe boxplots look very alike:\n\nggplot(dt, aes(x = category, y = value)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nHowever, the violin plot give away the entire story - namely the fact that the first category has two peaks (i.e. comes from a bimodal distribution, using the proper statistical terms), and it is unlikely that we observe the median value. A good example of such distributions from the political realm are the self-identification on the left-right ideological scale, with majority of respondents reporting to be either slightly conservative or slightly liberal.\n\nggplot(dt, aes(x = category, y = value)) + \n  geom_violin()",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#scatter-plots",
    "href": "07-data_visualization.html#scatter-plots",
    "title": "Data Visualization",
    "section": "Scatter plots",
    "text": "Scatter plots\nSo far we have described multiple ways of visually representing a continous variable, as well as the relationship between a continous variable and some discrete grouping variables. Under many circumstances, we are interested in examining the relationship between two continuous variables. One of the most commonly employed data visualization techniques for problems like this are scatter plots, which simply describe each observation as a point marked on a two-dimensional graph, with x axis representing one variable and y axis - the other. This can be seen in the example below, in which we look at the relationship between the professor’s experience approximated by years since attaining PhD and his or her salary.\n\nggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + \n  geom_point()\n\n\n\n\n\n\n\n\nIt is clear that we can see a some sort of relationship between the experience and earnings. In other words, there’s a degree of positive correlation between these two variables (we will examine this term more in depth in in the next chapter), i.e. professors with more experience tend to earn more on average. However, with experience the relationship becomes more noisy, as there’s more variation in earnings of the more experienced academics. As in the previous cases, we can specify additional aesthetics such as color or shape:\n\nggplot(Salaries, aes(x = yrs.since.phd, y = salary, \n                     color = rank, shape = rank)) + \n  geom_point()",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#line-of-best-fit",
    "href": "07-data_visualization.html#line-of-best-fit",
    "title": "Data Visualization",
    "section": "Line of best fit",
    "text": "Line of best fit\nWe can also add a line of best fit to our plot, by using the geom_smooth() geom, with the method argument specified to 'lm'. The line of best fit is simply the line that is closest to each of the points in the plot on average. The 'lm' methods stands for linear model - a method used for fitting such a line, described in more detail in the next part, in chapter on linear regression.\n\nggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + \n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nHere, you should be able to fully understand the difference between local and global aesthetic mappings mentioned earlier in the chapter. Notice the difference between providing the aesthetic mapping at the global level (i.e. to the ggplot() function call)…:\n\nggplot(Salaries, aes(x = yrs.since.phd, y = salary, color = rank)) + \n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\n… and just to the geom_point().\n\nggplot(Salaries, aes(x = yrs.since.phd, y = salary)) + \n  geom_point(aes(color = rank)) +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nIn the former case, the color = sex argument is used by both geom_point and geom_line(), which leads to 3 lines of different colors being fitted for each of the groups separately. When the color aesthetic mapping is only provided to the geom_point at the local level, one line is fitted to the data, as specified by the global mapping. Both of these approaches may be useful, depending on our research question. The comparison of these two plots further reveals an interesting behaviour in our data: when all the observations are considered toegether, there seems to be a positive relationship between years since PhD and salary. However when we look at the group level, the relationship appears to disappear - this behaviour is known as the Simpson’s Paradox. In this case, it’s primarily the result of the relationship between the rank and years since PhD - professors with higher ranks have more experience. When the variation in the salaries within those groups is considered, the experience measured in years seems to hardly explain any of the variation in earnings.",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#line-connecting-points",
    "href": "07-data_visualization.html#line-connecting-points",
    "title": "Data Visualization",
    "section": "Line connecting points",
    "text": "Line connecting points\nIn some cases, we might want to connect points in our figure with a line to depict some sort of trend in the mean of a variable depending on a categorical group that has a natural ordering - for example, suppose we want to visually examining differences in the average earnings of professors in our dataset depending on their years in service.\nFirst we can obtain the average for each group using group_by and summarise.\n\nbyage &lt;- Salaries %&gt;%\n  mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;%\n  group_by(yrs_range) %&gt;%\n  summarise(avg = mean(salary, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nWe can then plot the averages using geom_point:\n\nggplot(byage, aes(x = yrs_range, y = avg)) + \n  geom_point()\n\n\n\n\n\n\n\n\nWhile there seems to be some sort of trend in the data (even though the further points divert from it, which is likely the result of small sample size in a given group leading to rather unreliable sample estimates), the plot does not emphasize it enough. To make it more meanigful, we can add a line connecting the points with geom_line. Note that we have to set the group aesthetic to 1, which provides ggplot a hint on how to connect the point.\n\nggplot(byage, aes(x = yrs_range, y = avg, group = 1)) + \n  geom_point() + \n  geom_line()\n\n\n\n\n\n\n\n\nWe can extend the following example by comparing the same trend depending on the paid classes taken:\n\nbyage &lt;- Salaries %&gt;%\n  mutate(yrs_range = cut_width(yrs.since.phd, 5, boundary = 5)) %&gt;%\n  group_by(yrs_range, discipline) %&gt;%\n  summarise(avg = mean(salary, na.rm = TRUE)) %&gt;%\n  ungroup()\n\nggplot(byage, aes(x = yrs_range, y = avg, color = discipline, \n                  lty = discipline, group = discipline)) + \n  geom_point() +\n  geom_line()",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#faceting",
    "href": "07-data_visualization.html#faceting",
    "title": "Data Visualization",
    "section": "Faceting",
    "text": "Faceting\nFaceting refers to simply splitting the data by the value of a categorical variable, similarily to the way group_by treats the data when performing regular manipulation. The syntax is relatively simple: we simply add another layer to our plot called facet_wrap and specify the faceting variable following the tilde symbol ~ - this is a commonly used notation in R, indicating some sort of dependence of one variable on another - you will encounter it in the next two chapters.\n\nggplot(Salaries, aes(yrs.since.phd, salary)) + \n  facet_wrap(~ rank) + \n  geom_point() + \n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nAs a result we can see the scatter plots and lines of best fit in three separate plots, each one with an appropriate heading indicating the group of the observations. However, the output is difficult to read, due to the fact that the x-axis in the plot is fixed to the same length for each of the plots - this improves comparability of the figures, but limits its clarity. This can be changed by specifying an additional argument to the facet_wrap layer:\n\nggplot(Salaries, aes(yrs.since.phd, salary)) + \n  facet_wrap(~ rank, scales = 'free_x') + \n  geom_point(aes(color = sex)) + \n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n\nAgain, we can see that the relationship seen in the overall dataset disappears within each of the groups.\nWe can also facet using more than one variable and arrange the figures into a grid via a two dimensional grouping. For example, suppose we want to evaluate the distribution of the experience wihtin each combination of rank and discipline of the individuals. This can be achieved by using the facet_grid layer.\n\nggplot(Salaries, aes(x = yrs.since.phd)) + \n  facet_grid(discipline ~ rank) +\n  geom_histogram()\n\n\n\n\n\n\n\n\nIn here, we can see that professors in theoretical disciplines seem to have worked longer on average, however the range of the values is comparable in both cases. There also appears to be more Assistant Professors and Associate Professors in the applied departments, with no siginificant differences in gender. Similar analysis applied to earnings, however, reveals that the earnings seem to be higher at each level in the applied departments.\n\nggplot(Salaries, aes(x = salary)) + \n  facet_grid(discipline ~ rank) +\n  geom_histogram()",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "07-data_visualization.html#plotting-in-a-grid",
    "href": "07-data_visualization.html#plotting-in-a-grid",
    "title": "Data Visualization",
    "section": "Plotting in a grid",
    "text": "Plotting in a grid\nSometimes, we want to make our figure arrangement more customized, not necessarily arranged in accordance with our variable. For example, we may want to depict a consince summary of the most important of our findings in one 2x2 plot. To do this, we can use the plot_grid function from the cowplot package (which, as always, needs to be installed first).\n\nlibrary(cowplot)\n\nTo do arrange the plots in an arbitrary rectangular grid, we first need to create each of our plots as a separate ggplot object. To avoid repetition, however, it’s best to first create a basic object and then add new layers to it and save it under separate names.\n\nplt &lt;- ggplot(Salaries, aes(fill = discipline, color = discipline))\nplt_hist_s &lt;- plt + geom_density(aes(x = salary), alpha = 0.3)\nplt_hist_y &lt;- plt + geom_density(aes(x = yrs.since.phd), alpha = 0.3)\nplt_bar &lt;- plt + geom_bar(aes(y = rank))\nplt_sct &lt;- plt + geom_point(aes(x = yrs.since.phd, y = salary)) +\n  geom_smooth(aes(x = yrs.since.phd, y = salary), method = 'lm')\nplot_grid(plt_sct, plt_hist_s, plt_hist_y, plt_bar)",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html",
    "href": "08-statistical_association.html",
    "title": "Statistical Inference and Association",
    "section": "",
    "text": "Introduction\nSo far, we have described variety of methods to wrangle and explore data in R. After performing cleaning and exploratory analysis, it is common to perform more formal tests on the data to verify whether the relationships we hypothesized hold under the scrutiny of tests supported by statistical theory. In this chapter we will present couple of testing such relationships. While some formal knowledge of statistical inference will be useful, the chapter does not rely on your background knowledge to heavily.",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html#is-mean-different-from-0---one-sample-t-test",
    "href": "08-statistical_association.html#is-mean-different-from-0---one-sample-t-test",
    "title": "Statistical Inference and Association",
    "section": "Is mean different from 0? - one sample t-test",
    "text": "Is mean different from 0? - one sample t-test\nWe’ll start with a simple case. The data in the file wages.csv contains the information on the average salary of a sample of 100 individuals in 2005 and2 2010. We are interested in whether there was any growth in their salary. First, we read the data and obtain the percentege growth:\n\nlibrary(tidyverse)\nwages &lt;- read.csv('data/wages.csv')\nwages &lt;- wages %&gt;% mutate(change = (salary2010 - salary2005)/salary2005)\n\nWe can then examine the average:\n\nmean(wages$change)\n\n[1] 0.01489452\n\n\nWe see that the mean is around 0.015. Is that sufficient to claim that the average salary of employees in our target population has grown between 2005 and 2010? To check that, we can perform the so-called t-test, which takes into account the variability of our sample and its size, and determines the probability that we falsely reject the hypothesis that the mean is equal to 0 - this probability is the p-value. Note that p-value is not the probability of the mean being equal to 0. For those familiar with frequentist statistical inference, recall that the p-value is the probability of obtaining a statistic more extreme than the sample statistic in under the assumption that the null hypothesis is true in repeated sampling.\nTo conduct the t-test, we can simply use the t.test function:\n\nt.test(wages$change)\n\n\n    One Sample t-test\n\ndata:  wages$change\nt = 2.7211, df = 99, p-value = 0.007688\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.004033511 0.025755527\nsample estimates:\n mean of x \n0.01489452 \n\n\nThe p-value is usually the most important part of the output produced by the function. In this case, it is 0.0077. This means, that the mean is significantly different from 0 at the 95% confidence level. Confidence level is a way in which we can quantify our certainty in the difference of the parameter from some hypothesized value (in this case 0). The 95% confidence level is one of the most commonly used thresholds in the social sciences. Generally, the recipe for checking if a p-value satisfies a cerain confidence level is to subtract our confidence level from 1 and compare it with the p-value. If the p-value is smaller than the value obtained in the subtraction, the significance of the test statistic satisfies it. So, in our example, for 95% confidence \\(1 - 0.95 = 0.05\\) and 0.0077 $ &lt; 0.05$, thus we can say the the mean is significantly different from 0 at the 95% confidence level.\nAnother important information reported by the output of the t-test is the confidence interval. The confidence interval provides us with a way of quantifying our uncertainty about the underlying value of a statistic (in our case the mean), given the variability of our sample (measured by the standard deviation, as described in chapter 4. Suppose we take 100 random samples from our original population. The 95% confidence interval describes the range into which we exepct the mean of 95 out of 100 samples to fall. In case of the mean, it is given by \\(\\bar{\\mu}\\pm 1.96SE(\\bar{\\mu})\\), where \\(\\bar\\mu\\) is the sample mean we obtained and \\(SE(\\bar{\\mu})\\) is the standard error of the mean given by \\(\\sigma/\\sqrt{n}\\), which is the sample standard deviation divided by the square root of the number of the observations in our sample. Note that 95% confidence interval not including 0 is equivalent with the statistic being significantly different from 0 at the 95% level.",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html#error-bars",
    "href": "08-statistical_association.html#error-bars",
    "title": "Statistical Inference and Association",
    "section": "Error bars",
    "text": "Error bars\nThe confidence interval around the mean can be displayed in ggplot using the geom_errorbar function. The so-called errorbar is a good way to display the range of the likely values around the mean.\nIn case of our data, to plot the errorbars, we first compute the standard error of the mean using the formula discussed above.\n\nwages_summary &lt;- wages %&gt;%\n  summarise(change_mean = mean(change), \n            change_err = sd(change)/sqrt(n()))\n\nWe than create a bar plot and add the geom_errorbar on top of it. Note that in this case we specify the x aesthetic as a chraracter 'Wage change'. This is because it is unusual to plot just one bar in the plot, and geom_col and geom_errorbar require some x aesthetic to be passed.\n\nggplot(wages_summary, aes(x = 'Wage change', y = change_mean)) +\n  geom_col(width = 0.2) + \n  geom_errorbar(aes(ymin = change_mean - 1.96 * change_err, \n                ymax = change_mean + 1.96 * change_err), width = 0.1) + \n  labs(x = '', y = 'Mean change in wage')",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html#are-two-means-different---two-sample-t-test",
    "href": "08-statistical_association.html#are-two-means-different---two-sample-t-test",
    "title": "Statistical Inference and Association",
    "section": "Are two means different? - two sample t-test",
    "text": "Are two means different? - two sample t-test\nBy simple extension, we can apply the logic of the t-test to the difference between two means within one sample - this is so called a two sample t-test and is implemented in R by the same function. The grouping variable is specified using the formula notation, which we will encounter in the next chapter when dealing with linear regression. To test the difference in mean of variable x between groups defined by y, we specify t.test(x ~ y). So, coming back to our example, we can examine whether the average salary in 2005 is different for male and female employees.\n\nt.test(salary2005 ~ gender, data = wages)\n\n\n    Welch Two Sample t-test\n\ndata:  salary2005 by gender\nt = -2.4595, df = 97.494, p-value = 0.01568\nalternative hypothesis: true difference in means between group F and group M is not equal to 0\n95 percent confidence interval:\n -19486.287  -2082.285\nsample estimates:\nmean in group F mean in group M \n       31020.72        41805.01 \n\n\nThe result seem to be significant, meaning that we have statistical evidence that there is a real difference between male and female earnings in the underlying populations (or more precisely, that if we took 100 samples from the underlying population, more than 95 of them would show a non-zero difference). Based on our sample, however, the magnitude of that difference is quite difficult to determine, as we expect it to range between 19000 and 2000 dollars. We can visualize this again using geom_errorbar:\n\nwages %&gt;%\n  group_by(gender) %&gt;%\n  summarise(wage_mean = mean(salary2005), \n            wage_err = sd(salary2005)/sqrt(n())) %&gt;%\n  ggplot(aes(x = gender, y = wage_mean)) + \n  geom_col(width = 0.3) + \n  geom_errorbar(aes(ymin = wage_mean - wage_err, \n                    ymax = wage_mean + wage_err), width = 0.2) +\n  ylab('Average salary in 2005')",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html#covariance",
    "href": "08-statistical_association.html#covariance",
    "title": "Statistical Inference and Association",
    "section": "Covariance",
    "text": "Covariance\nSo far we have discussed some techniques for looking at statistical relationships between one continuous variable (such as wage) and one categoric variable (such as salary). Under many circumstances, however, we would be interested in measuring the association between two continuous variables. In this part we will consider data on a sample of students with free variables: average weekly time spent studying, days a student has been home ill and achieved GPA.\n\nscores &lt;- read_csv('data/scores.csv')\n\nWe can examine pairwise relationships between the variables using the pairs() function, which is a useful tool when analyzing datasets with multiple continuous variables.\n\npairs(scores,upper.panel = NULL)\n\n\n\n\n\n\n\n\nLet’s start by interpreting this plot. The rectangles with variable tell as the name of the axis. For example, the plot in the lower left corner shows the relationship between the GPA (x axis) and study time (y axis). We can see that students in our sample who spent more time studying seem to have had higher grades - points higher on one axis tend to also be higher on the other. On the other hand, students who were ill more had, on average, achieved lower grades. There seems to be no relationship between being ill and time spent studying.\nIn statistical terms, this relationship can be measured with covariance between these two variables. Recall our discussion of variance in the exploratory analysis chapter. We calculated it for vector x using sum((x - mean(x))^2)/(length(x) - 1), or the var(x) function for short. Covariance is essentially the same, but calculated for two variables instead of one, using sum((x - mean(x)) * mean((y - mean(y))))/(length(x) - 1) or the cov function for short. You can see that demonstrated below:\n\ncov_fun &lt;- function(x, y) sum((x - mean(x)) * (y - mean(y)))/(length(x) - 1)\ncov_fun(scores$gpa, scores$study_time)\n\n[1] 1.337834\n\ncov(scores$gpa, scores$study_time)\n\n[1] 1.337834\n\n\nThe variance is actually nothing else but covariance of a variable with itself!\n\ncov(scores$gpa, scores$gpa) == var(scores$gpa)\n\n[1] TRUE\n\n\nEssentially covariance measures the average of element-wise product of each observations deviation from its mean for two variables. This may sound complicated, but breaking it down makes it simple. In our two variables, gpa and days_ill, people who have grades higher than the average grade also tend to study longer than the average study time. So, if for all students we subtract the mean grade and mean study time from their respective grades and study times, the average of that will be positive.\n\ncov(scores$gpa, scores$study_time)\n\n[1] 1.337834\n\n\nIf, on the other hand people who have higher grades than average tend to have lower time spent ill at home than average, the average of the products of these demeaned (i.e. with the mean subtracted from each) variables will be negative, since the majority of the products will be negative.\n\ncov(scores$gpa, scores$days_ill)\n\n[1] -11.16068\n\n\nFinally, if there’s no relationship between two variables - for example between days_ill and study_time, we expect the average of the products of the demeaned variables to be close to 0, since around the same number will be positive and negative.\n\ncov(scores$study_time, scores$days_ill)\n\n[1] 2.419399\n\n\nThe last example seems to be incorrect? How is that possible that the covariance between study_time and days_ill is larger than the covariance between study_time and gpa, even though the plot suggests a much stronger relationship between the latter? We will explore this in the next section on correlation.",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "08-statistical_association.html#correlation",
    "href": "08-statistical_association.html#correlation",
    "title": "Statistical Inference and Association",
    "section": "Correlation",
    "text": "Correlation\nThe downside of covariance as a measure of relationship is that it is completely dependent on the units of the two variables measures, or, more percisely, the range of values they take. If we take covariance average time spent studying and GPA and compare it with the covariance between average time spent studying and time ill, the latter will be greater, simply because time is expressed in hours and days, with values ranging between 0.8 and 15.1 and 1 and 110 respectively, while GPA ranges between 1 and 5:\n\nvapply(scores, range, numeric(2))\n\n     study_time days_ill  gpa\n[1,]       1.42        4 1.33\n[2,]      15.28      100 4.70\n\n\nSo, the values we get by multiplying study_time and days_ill will tend to be higher in general. This makes the use of covariance problematic in estimating the objective strength of the relationship between two variables.\nIn order to avoid it, we use correlation instead. Correlation is simply covariance, but divided by the product of the standard deviation of each of the variables. Without going into the mathematical details, suffice to say that it standardizes the covariance, so that it ranges between -1 and 1, -1 meaning perfect negative relationship (i.e. when one variable increases, the other always decreases) and 1 meaning perfect positive relationship (when one variable increases, the other decreases). 0 means that there’s no relationship between these variables.\nWe can see that again using R code. cor_fun is created for demonstration purposes, but the usual function used in R to obtain correlation is simply cor.\n\ncor_fun &lt;- function(x, y) cov_fun(x, y)/(sd(x) * sd(y))\ncor_fun(scores$gpa, scores$study_time)\n\n[1] 0.5538811\n\ncor(scores$gpa, scores$study_time)\n\n[1] 0.5538811\n\n\nNow we can see, that the relationship between study_time and days_ill is indeed close to 0:\n\ncor(scores$study_time, scores$days_ill)\n\n[1] 0.03523657\n\n\nWe can also use cor on entire data frame to obtain the so-called correlation matrix, which shows pairwise relationships between two variables\n\ncor(scores)\n\n           study_time    days_ill        gpa\nstudy_time 1.00000000  0.03523657  0.5538811\ndays_ill   0.03523657  1.00000000 -0.7001128\ngpa        0.55388114 -0.70011285  1.0000000\n\n\nNote that the diagonal is always 1, i.e. the correlation of a variable with itself is always 1.\nFor obvious reasons, the function also won’t work if applied to a data frame containing any text data:\n\ncor(wages)\n\nTo visualize pariswise relationships between two variables using a plot that is a bit more sophisticated that the one produced by pairs() we can use the ggpairs function from the GGally package, which produced a ggplot style plot, presenting correlations, density plots and scatter plots of all the numeric variables in our dataset:\n\nlibrary(GGally)\nggpairs(scores)",
    "crumbs": [
      "DATA ANLYSIS",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical Inference and Association</span>"
    ]
  },
  {
    "objectID": "09-ols.html",
    "href": "09-ols.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Introduction\nRegression is the power house of the social sciences. It is widely applied and takes many different forms. In this Chapter we are going to explore the linear variant, also called Ordinary Least Squares (OLS). This type of regression is used if our dependent variable is continuous. In the following Chapter we will have a look at regression with a binary dependent variable and the calculation of the probability to fall into either of those two categories. But let’s first turn to linear regression.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#the-theory",
    "href": "09-ols.html#the-theory",
    "title": "Linear Regression",
    "section": "The Theory",
    "text": "The Theory\nRegression is not only able to identify the direction of a relationship between an independent and a dependent variable, it is also able to quantify the effect. Let us choose Y as our dependent variable, and X as our independent variable. We have some data which we are displaying in a scatter plot:\n\nWith a little goodwill we can already see that there is a positive relationship: as X increases, Y increases, as well. Now, imagine taking a ruler and trying to fit in a line that best describes the relationship depicted by these points. This will be our regression line.\nThe position of a line in a coordinate system is usually described by two items: the intercept with the Y-axis, and the slope of the line. The slope is defined as rise over run, and indicates by how much Y increases (or decreases is the slope is negative) if we add an additional unit of X. In the notation which follows we will call the intercept \\(\\beta_{0}\\), and the slope \\(\\beta_{1}\\). It will be our task to estimate these values, also called coefficients. You can see this depicted graphically here:\n\nPopulation\nWe will first assume here that we are dealing with the population and not a sample. The regression line we have just drawn would then be called the Population Regression Function (PRF) and is written as follows:\n\\[\\begin{equation}\nE(Y|X_{i}) = \\beta_{0} + \\beta_{1} X_{i}\n(\\#eq:prf)\n\\end{equation}\\]\nBecause wer are dealing with the population, the line is the geometric locus of all the expected values of the dependent variable Y, given the values of the independent variables X. This has to do with the approach to statistics that underpins this module: frequentist statsctics (as opposed to Bayesian statistics). We are understanding all values to be “in the long run”, and if we sampled repeatedly from a population, then the expected value is the value we would, well, expect to see most often in the long run.\nThe regression line is not intercepting with all observations. Only two are located on the line, and all others have a little distance between them and the PRF. These distances between \\(E(Y|X_{i})\\) and \\(Y_{i}\\) are called error terms and are denoted as \\(\\epsilon_{i}\\).\n\nTo describe the observations \\(Y_{i}\\) we therefore need to add the error terms to equation @ref(eq:prf):\n\\[\\begin{equation}\nY_{i} = \\beta_{0} + \\beta_{1} X_{i} + \\epsilon_{i}\n(\\#eq:pop)\n\\end{equation}\\]\nSample\nIn reality we hardly ever have the population in the social sciences, and we generally have to contend with a sample. Nonetheless, we can construct a regression line on the basis of the sample, the Sample Regression Function (SRF). It is important to note that the nature of the regression line we derive fromt he sample will be different for every sample, as each sample will have other values in it. Rarely, the PRF is the same as the SRF - but we are always using the SRF to estimate the PRF.\nIn order to flag this up in the notation we use to specify the SRF, we are using little hats over everything we estimate, like this:\n\\[\\begin{equation}\n\\hat{Y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i}\n(\\#eq:srf)\n\\end{equation}\\]\nAnalogously, we would would describe the observations \\(Y_{i}\\) by adding the estimated error terms \\(\\hat{\\epsilon}_{i}\\) to the equation.\n\\[\\begin{equation}\nY_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{i} + \\hat{\\epsilon}_{i}\n\\end{equation}\\]\nThe following graph visdualised the relationship between an observation, the PRF, the SRF and the respective error terms.\n\nOrdinary Least Squares (OLS)\nWhen you eye-balled the scatter plot at the start of this Chapter in order to fit a line through it, you have sub-consciously done so by minimising the distance between each of the observations and the line. Or put differently, you have tried to minimise the error term \\(\\hat{\\epsilon}_{i}\\). This is basically the intuition behind fitting the SRF mathematically, too. We try to minimise the sum of all error terms, so that all observations are as close to the regression line as possible. The only problem that we encounter when doing this is that these distances will always sum up to zero.\nBut similar to calculating the standard deviation where the differences between the observations and the mean would sum up to zero (essentially we are doing the same thing here), we simply square those distances. So we are not minimising the sum of distances between observations and the regression line, but the sum of the squared distances between the observations and the regression line. Graphically, we would end up with little squares made out of each \\(\\hat{\\epsilon}_{i}\\) which gives the the method its name: Ordinary Least Squares (OLS).\n\nWe are now ready to apply this stuff to a real-world example!",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#the-application",
    "href": "09-ols.html#the-application",
    "title": "Linear Regression",
    "section": "The Application",
    "text": "The Application\nIn the applied part of this Chapter, we are going to model the feelings towards Donald Trump in the lead-up to the presidential election 2020. Data for this investigation are taken from https://electionstudies.org/data-center/2020-exploratory-testing-survey/ Please follow this link and download the “2020 Exploratory Testing Survey” and pop it into a working directory.\nWe can then load the ANES data set:\n\nanes &lt;- read.csv(\"data/anes.csv\")\n\n\nsummary(anes$fttrump1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    0.00   40.00   44.59   80.00  999.00 \n\n\nThis is no good, as the variable is bounded between 0 and 100. In fact 999 is a placeholder for missing data throughout the data set. We need to replace this with NAs.\n\nanes[anes == 999] &lt;- NA\n\nIf we look at the summary again, everything looks fine now:\n\nsummary(anes$fttrump1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00    0.00   40.00   42.42   80.00  100.00       7 \n\n\nBy the way, had you just wanted to replace this in one variable, for example only in fttrump1, you could have called:\n\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA))\n\nNorris and Inglehart (2016) have argued that:\n\npopulist support in Europe is generally stronger among the older generation, men, the less educated, the religious, and ethnic majorities, patterns confirming previous research.\n\nLet’s see if this also applies to presidential elections in the US. We first look at the question: “Do older people rate Trump higher than younger people?”. Our independent variables is age.\n\nsummary(anes$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  18.00   34.75   49.00   48.87   63.00  110.00 \n\n\nLet’s evaluate the relationship through a scatter plot with a line of best fit:\n\nlibrary(tidyverse)\n\nggplot(anes, aes(x = age, y = fttrump1)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n\n\n\n\n\n\nThere is a positive relationship. We can calculate the exact numerical nature of that relationship as follows:\n\nmodel1 &lt;- lm(fttrump1 ~ age, data = anes)\n\nWe start by specifying an object into which we store the results. Then we call lm which means linear model. Our dependent variable fttrump1 is listed first, and then after a tilde the independent variable, age. Finally, we tell R which data set to use. We can then print the result, by calling model1.\n\nmodel1\n\n\nCall:\nlm(formula = fttrump1 ~ age, data = anes)\n\nCoefficients:\n(Intercept)          age  \n    31.6837       0.2197  \n\n\nHow would we interpret these results?\n\nAt an age of zero, a person would rate Trump at 31.68 on average. This of course makes little sense in anything but a theoretical / mathematical consideration.\nWith every additional year of age, a person would rate Trump 0.22 points higher on average.\n\nBut are these findings significant at an acceptable significance level? Let’s find out, by getting a more detailed output:\n\nsummary(model1)\n\n\nCall:\nlm(formula = fttrump1 ~ age, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.847 -39.152  -0.523  39.258  64.143 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.68372    2.14801  14.750  &lt; 2e-16 ***\nage          0.21967    0.04157   5.284 1.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.67 on 3071 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.009011,  Adjusted R-squared:  0.008688 \nF-statistic: 27.92 on 1 and 3071 DF,  p-value: 1.35e-07\n\n\nOK, there is a lot more here, and it is worth pausing to go through this step by step. First, R reminds us of the actual formula we have used to estimate the model:\n\nI am ignoring the section on residuals, as we don’t need to make our life more difficult than it needs to be. Now come the coefficients:\n\nThe size and direction is of course the same as in our previous output, but this output now contains some additional information about the standard error, the resulting t-value, and the p-value. R is very helpful here, in that it offers us a varying amount of asterisks according to different, commonly accepted levels of significance. 0.05 is standard practice in the social sciences, so we will accept anything with one, or more asterisks. Both our intercept and the slope coefficient are significant at a 95% confidence level, so we have shown that there is a statistical relationship between age and ratings for Trump.\nI am omitting the residual standard error for the same reason as before, but let us look at the model fit indicators.\n\nMultiple R-Squared (aka \\(R^{2}\\)) tells us how much variation in the dependent variable fttrump1 is explained through the independent variable age. \\(R^{2}\\) runs between 0 and 1, where 1 is equal to 100% of the variation. In our case, we have explained a mere 0.09% of the Trump rating. This is lousy, and we can do a lot better than that. Never expect anything near 100% unless you work with a toy data set from a text book. If you get 60-70% you can be very happy. I will return to Adjusted \\(R^{2}\\) in the Section on Multiple Linear Regression.\nThe F-statistic at the end:\n is a test with the null hypothesis that all coefficients of the model are jointly zero. In our case, we can reject this null hypothesis very soundly, as the p-value is far below the commonly accepted maximum of 5%.\n\nCategorical Independent Variables (aka ‘Dummies’)\nOften variables are categorical. One such example is the variable sex which has two categories: male and female.\n\nsummary(anes$sex)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   2.000   1.522   2.000   2.000 \n\ntable(anes$sex)\n\n\n   1    2 \n1473 1607 \n\n\nTurn this into a factor variable and assign telling labels\n\nanes$sex &lt;- factor(anes$sex, labels = c(\"Male\", \"Female\"))\n\nCheck if this has worked:\n\ntable(anes$sex)\n\n\n  Male Female \n  1473   1607 \n\n\nLet’s estimate the model:\n\nmodel2 &lt;- lm(fttrump1 ~ sex, data = anes)\nsummary(model2)\n\n\nCall:\nlm(formula = fttrump1 ~ sex, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.156 -38.992  -1.156  38.844  61.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   46.156      1.009  45.749  &lt; 2e-16 ***\nsexFemale     -7.165      1.397  -5.129 3.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.68 on 3071 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.008493,  Adjusted R-squared:  0.00817 \nF-statistic: 26.31 on 1 and 3071 DF,  p-value: 3.095e-07\n\n\nHow do we interpret this?\n\nLet’s do the slope coefficient first: a women would rate Trump at 7.16 points less than a man on average. The interpretation of a dummy variable coefficient is done with regards to the reference category. In our case this is “male”. So the effect we observe here is equivalent of moving from “male” to “female” and that effect adds 7.16 points.\nThis gives you an indication of how to interpret the intercept in this case: The value displayed is how men would rate Trump on average, namely at 46.16 points. All of this is significant at a 95% confidence level.\n\nThis effect corroborates the hypothesis advanced by Inglehart and Norris, but the results are not displayed in the most elegant way. The sex these authors made a statement about were men. So we need to change the reference category to “female”.\n\nanes &lt;- anes %&gt;%\n  mutate(sex = relevel(sex, ref = \"Female\"))\n\nWhen we re-estimate the model, we get the effect displayed directly:\n\nmodel2 &lt;- lm(fttrump1 ~ sex, data = anes)\nsummary(model2)\n\n\nCall:\nlm(formula = fttrump1 ~ sex, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-46.156 -38.992  -1.156  38.844  61.008 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  38.9919     0.9661  40.358  &lt; 2e-16 ***\nsexMale       7.1646     1.3969   5.129 3.09e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.68 on 3071 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.008493,  Adjusted R-squared:  0.00817 \nF-statistic: 26.31 on 1 and 3071 DF,  p-value: 3.095e-07\n\n\nWhilst many categorical variables are binary, of course not all of them are. So how does this work with a categorical variable with 3 or more levels?\nThe next determinant mentioned in Inglehart and Norris’ paper is education. We can obtain information about a respondent’s level of education with the variable educ.\n\nsummary(anes$educ)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   4.000   4.012   5.000   8.000 \n\ntable(anes$educ)\n\n\n  1   2   3   4   5   6   7   8 \n100 656 622 326 761 424 102  89 \n\n\nThis is not terribly telling in itself, yet, so let’s have a look at the codebook:\n\nThe first step is, as before to recode this variable into a factor variable:\n\nanes$educ &lt;- factor(anes$educ)\n\nEight levels are too many here to do any meaningful analysis, and two would be too reductionist. For sake of simplicity, let’s go with three: low, medium and high education. We recode into an ordered factor as follows:\n\nanes &lt;- anes %&gt;%\n  mutate(educ_fac = recode(educ, '1'=\"low\", \n                       '2'= \"low\",\n                       '3'= \"low\",\n                       '4' = \"medium\",\n                       '5' = \"medium\",\n                       '6' = \"high\",\n                       '7' = \"high\",\n                       '8' = \"high\"))\n\nCheck the results:\n\ntable(anes$educ_fac)\n\n\n   low medium   high \n  1378   1087    615 \n\n\nAnd we are ready to go:\n\nmodel3 &lt;- lm(fttrump1 ~ educ_fac, data = anes)\nsummary(model3)\n\n\nCall:\nlm(formula = fttrump1 ~ educ_fac, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.943 -42.019  -2.388  37.981  57.981 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     42.0189     1.0481  40.090   &lt;2e-16 ***\neduc_facmedium   0.9240     1.5775   0.586    0.558    \neduc_fachigh     0.3693     1.8870   0.196    0.845    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.85 on 3070 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.0001119, Adjusted R-squared:  -0.0005395 \nF-statistic: 0.1718 on 2 and 3070 DF,  p-value: 0.8422\n\n\nWhilst the intercept is statistically significant, the slope coefficients are not. Therefore, we can conclude that education has no statistical influence on Trump’s approval ratings.\nNote, that as in the sex example before, R has chose the first level of the independent variable as the reference category. If you wish to change this, you can do so in the same manner as before. You can also check which level R has used as the reference category with the contrasts() command. Here:\n\ncontrasts(anes$educ_fac)\n\n       medium high\nlow         0    0\nmedium      1    0\nhigh        0    1\n\n\n\n\nSummary for Bivariate Regression\nAnd that’s it! You have made the first big step to understanding regression output and producing such output yourself. But explanations in the real world are never mono-causal. There are always multiple influences working at the same time, and we need to set up our statistical model to take this complexity into account. Which brings us to the next step: multiple regression.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#the-theory-1",
    "href": "09-ols.html#the-theory-1",
    "title": "Linear Regression",
    "section": "The Theory",
    "text": "The Theory\nWe are simply extending Equation @ref(eq:srf) by adding more independent variables. For two independent variables we would write:\n\\[\\begin{equation}\n\\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{1i} + \\hat{\\beta}_{2} X_{2i}\n\\end{equation}\\]\nNote that not only the betas have a subscript now, but also the independent variables. For example \\(X_{1i}\\) would denote the \\(i^{th}\\) observation of independent variable 1.\nWe can extend this more generally to:\n\\[\\begin{equation}\n\\hat{Y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} X_{1i} + \\hat{\\beta}_{2} X_{2i} + ... + \\hat{\\beta}_{n} X_{ni}\n(\\#eq:srffull)\n\\end{equation}\\]\nwhere n is the number of independent variables in the model.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#the-application-1",
    "href": "09-ols.html#the-application-1",
    "title": "Linear Regression",
    "section": "The Application",
    "text": "The Application\nJust as we have extended Equation @ref(eq:srf) to @ref(eq:srffull), we can extend our model in R - we simply need to connect the independent variables with +. If we wished to look at the joint influence of all independent variables we have included so far, we would type:\n\nmodel4 &lt;- lm(fttrump1 ~ age + sex + educ_fac, data = anes)\nsummary(model4)\n\n\nCall:\nlm(formula = fttrump1 ~ age + sex + educ_fac, data = anes)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-55.109 -38.373  -1.385  37.536  68.242 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    27.33647    2.40776  11.353  &lt; 2e-16 ***\nage             0.23274    0.04156   5.599 2.34e-08 ***\nsexMale         7.66785    1.40848   5.444 5.62e-08 ***\neduc_facmedium  0.32209    1.56889   0.205    0.837    \neduc_fachigh   -0.36738    1.89537  -0.194    0.846    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.5 on 3068 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.01868,   Adjusted R-squared:  0.0174 \nF-statistic:  14.6 on 4 and 3068 DF,  p-value: 8.15e-12",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#the-interpretation",
    "href": "09-ols.html#the-interpretation",
    "title": "Linear Regression",
    "section": "The Interpretation",
    "text": "The Interpretation\nAs we have included multiple independent variables now, the interpretation of coefficients changes slightly. The principle here is called “Ceteris Paribus” which means “all other things being equal”. What exactly does that mean?\nTake the coefficient for age, for example. If this was a bivariate regression we would interpret it as follows: “on average, for every additional year of age, the support for Trump would increase by 0.23274 units”. The first thing you will note here is that the size of this coefficient has changed. When we ran the bivariate model it was 0.21967. The reason is the inclusion of other variables. By doing this, we are isolating, or purifying the influence our variable age has, holding sex and educ_fac constant. You could also say “sex and educ_fac being equal, on average, for every additional year of age, the support for Trump would increase by 0.23274 units”. This is our new interpretation of coefficients in multiple regression.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#model-fit-again",
    "href": "09-ols.html#model-fit-again",
    "title": "Linear Regression",
    "section": "Model Fit (again)",
    "text": "Model Fit (again)\nIf I added “shoe size of interviewee” into the model, this would make absolutely no sense from a theoretical point of view. Yet, our R-Squared would either stay the same, or even increase. R-Squared can never decrease from the addition of more variables. This is of course no good. We need a measure that takes into account the number of independent variables, and penalises us for the inclusion of irrelevant variables. This measure is called “Adjusted R-Squared” and can be found at the bottom of the model summary.\nIf you run a bivariate model, always use “Multiple R-Squared”, when running multiple regression, always use “Adjusted R-Squared”. This will allow you to compare the model fit between models and to determine whether a variable adds explanatory power (“Adjusted R-Squared” increases), is pointless (“Adjusted R-Squared” stays the same), or is detrimental to the model (“Adjusted R-Squared” decreases).",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "09-ols.html#model-specification",
    "href": "09-ols.html#model-specification",
    "title": "Linear Regression",
    "section": "Model Specification",
    "text": "Model Specification\nYou will have noticed that I quoted Norris and Inglehart’s determinants for populist support and I have done so intentionally. The selection of your independent variables always needs to be guided by theory. Theory provides a logic and a structure to our enquiry which makes it scientific (which is why what we do is called “Political Science” and not “Politics”, and let nobody tell you otherwise. Politics takes place in Westminster and is defined as the negotiation between different groups according to their respective power - don’t say you didn’t learn anything on this module). Otherwise, we would only be stabbing into the dark, randomly checking factors we deem influential.\nThe composition of a regression model therefore also needs to be guided by theory. If a theory has multiple components, or has been extended over time, you can test the propositions of each stage separately. Classical Modernisation for example, posits that wealthier countries are more democratic. Later on, influencing factors were extended to health, education, urbanisation, etc. So it would make sense to run a model with just GDP first, and then to add the other three variables.\nOccasionally, the inclusion of new variables takes the significance away from previously included ones, and you are able to show that these new variables explain the variation in the dependent variable better. So, it always makes sense to test different combinations of independent variables to look deeper into which explains the outcome better / best.\nOne last word on selecting independent variables: you can’t just throw a random number of variables into the model. you need to observe the principle of parsimony which asks you to use as many variables as necessary, but as few as possible.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Linear Regression</span>"
    ]
  },
  {
    "objectID": "10-logit.html",
    "href": "10-logit.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "Introduction\nIn the previous Chapter we encountered linear regression analysis which allowed us to quantify the amount and direction of one or more independent variables on a continuous dependent variable. I already mentioned there, that there is also a type of a regression which can deal with a binary dependent variable. This is usually a yes/no scenario, such as democracy / autocracy, war / peace, trade agreement / no trade agreement, … You get the picture. Many problems or questions in political science have binary outcomes, and so you are about to learn a very important and useful method to answer research questions. As in the previous Chapter, I will take you some through some theory first, and then we are applying the theory to an empirical example. This time concerning the survival of passengers on the Titanic.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "10-logit.html#footnotes",
    "href": "10-logit.html#footnotes",
    "title": "Logistic Regression",
    "section": "",
    "text": "For probit, the normal distribution is used. This would lead to very similar results.↩︎\nSource: https://stackoverflow.com/questions/32040504/regression-logistic-in-r-finding-x-value-predictor-for-a-particular-y-value↩︎",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "11-markdown.html",
    "href": "11-markdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "Introduction\nOne of the more persuasive arguments to use R (over say SPSS) is its ability to easily make work reproducible. This means that you could give your RScript to another person, and they would be able to replicate, step by step your data preparation and analysis, obtaining the same results. This is not only a fundamental part of any scientific enquiry, but also helpful to you, should you wish to replicate your own work at a later stage. Take it from me: after a few months you will have forgotten any data management procedure or steps used in an analysis. How do you achieve this? You can, of course, use annotations in your RScript to explain to others and yourself what you have done in each step. And in fact you should do exactly that as a matter of routine. But we can go a step further than that.\nYou might have asked yourself when studying the previous Chapters how to get all the great output in the form of Tables and Figures into an essay, or your dissertation. The answer to this is Markdown. It lets you create reproducible essays / articles with great ease, and even has a feature to create your bibliography and take care of your referencing. Intrigued? Then read on!\nSo, what is R Markdown? As promised in the introduction,",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#the-yaml",
    "href": "11-markdown.html#the-yaml",
    "title": "R Markdown",
    "section": "The YAML",
    "text": "The YAML\n\nAcronym for “Yet Another Markup Language”\nContains the settings for the entire document\nPrimarily parameters and bibliography",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#text",
    "href": "11-markdown.html#text",
    "title": "R Markdown",
    "section": "Text",
    "text": "Text\nWriting\n\nBasically, you can just write the text as you always would.\nIt’s “only” the formatting that differs.\n\nCompiling\n\nMarkdown is not WYSIWYG (What You See Is What You Get)\nCompiling is called “Knitting”\n\n\n\n\n\n\n\n\n\n\n\nHeadings\n# Heading \n\n## Sub-Heading\n\n### Sub-Sub Heading\n\n\nEmphasis\n*italic*\n\n**bold**\n\n\\texttt(courier)\n\n\\underline(underline)\n\n\nLinks\n\nJust insert the link\n\n\n\nLists\n- \n    - \n    - \n\n\nNumbered Lists\n1. \n    a. \n    b. \n2. \n\n\nLine Breaks\nThis will not produce a line break:\nline 1\nline 2\nBut this will:\nline 1\n\nline 2\n\n\nBlock Quotes\n\nSimply precede the quote with “\\(&gt;\\)”\nThe block will turn green when you do so\n\n\n\nEquations\n\nTwo ways to set equations\nEither wrapped in $ signs\n\n$ equation $\n\nor\n\n\\begin{equation}\nequation\n\\end{equation}\nExample\nThe command\n\\begin{equation}\nY = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon\n\\end{equation}\nresults in:\n\\[\\begin{equation}\n  Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon\n\\end{equation}\\]\nTo suppress the numbering you type:\n\\begin{equation*}\nY = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon\n\\end{equation*}\n\\[\\begin{equation*}\n  Y = \\beta_{0} + \\beta_{1} x_{i} + \\epsilon\n\\end{equation*}\\]\n\n\nList of Symbols\nYou can find a good compilation of symbols here:\nhttps://latex.wikia.org/wiki/List_of_LaTeX_symbols",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#code-chunks",
    "href": "11-markdown.html#code-chunks",
    "title": "R Markdown",
    "section": "Code Chunks",
    "text": "Code Chunks\n\nIn-Line R Code\nYou can include R Code in Markdown by wrapping it in\n```{r}\n\n```\nShortcut:\n\nMac: Option + Command + I\nWindows: Ctrl + Alt + I\n\nExample\n```{r}\n5+3\n```\nresults in\n\n5+3\n\n[1] 8\n\n\nChunk Options\nTypes of Output Suppressed by:\n\n\n\n\n\n\n\n\n\nExamples\nDisplay, but not calculate:\n```{r eval=F}\n5+3\n```\nSuppress Messages from Packages\n```{r message=F}\nlibrary(tidyverse)\n```\nSuppress Messages from Packages and only display\n```{r message=F, eval=F}\nlibrary(tidyverse)\n```\n\n\nFigures and Graphs\nYou can also use this to include figures and graphs:\n```{r echo=FALSE, out.width='75%'}\nknitr::include_graphics('./filename.png')\n```\nAdd a Caption!\n```{r echo=FALSE, out.width='75%', fig.cap=\"\\\\label{fig:test}Test Caption\"}\nknitr::include_graphics('./filename.png')\n```\nAnd then refer to it in the text with\n\\ref{fig:test}\nExample\n```{r echo=FALSE, out.width='75%', fig.cap=\"\\\\label{fig:spell}RStudio Task Bar\"}\nknitr::include_graphics('./spell.png')\n```\n\nAs we can see in \\ref{fig:spell}\nturns into\n\n\n\n\n\nRStudio Task Bar\n\n\n\n\nAs we can see in Figure 11.1",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#spell-checker",
    "href": "11-markdown.html#spell-checker",
    "title": "R Markdown",
    "section": "Spell-Checker",
    "text": "Spell-Checker",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#useful-commands",
    "href": "11-markdown.html#useful-commands",
    "title": "R Markdown",
    "section": "Useful Commands",
    "text": "Useful Commands\n\nNew page\n\n\\newpage\n\nCentering a Line\n\n\\begin{center}\nText to be centred.\n\\end{center}",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#getting-started",
    "href": "11-markdown.html#getting-started",
    "title": "R Markdown",
    "section": "Getting Started",
    "text": "Getting Started\n\nDownload and install:\n\nMac: MacTeX (http://www.tug.org/mactex/)\nWindows: MiKTeX (https://miktex.org/)\n\nThese contain a complete TeX system with LaTeX itself and editors to write documents.\n\nMore on: https://www.latex-project.org/get/\nInstructions\n\nPlace in the working directory the file “bibliography.bib”\nAdd a heading at the very end of the document called",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#citations-in-markdown",
    "href": "11-markdown.html#citations-in-markdown",
    "title": "R Markdown",
    "section": "Citations in Markdown",
    "text": "Citations in Markdown\nCitations\nText [@grolemund:2016, p. 361]  \n\nText [@grolemund:2016, pp. 33-35, 38-39 and *passim*].\nSuppress Author\nGrolemund and Wickham write that ... [-@grolemund:2016]  \nThis can be useful when the author is already mentioned in the text.",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "11-markdown.html#the-.bib-file",
    "href": "11-markdown.html#the-.bib-file",
    "title": "R Markdown",
    "section": "The .bib file",
    "text": "The .bib file\nEvery citation needs to have a reference in the .bib file, such as:\n@book{grolemund:2016,\n  author={Garrett Grolemund and Hadley Wickham},\n  title={R for Data Science},\n  publisher={O'Reilly Media},\n  year={2016}}\nYou can only edit this file in LaTeX\nIf you want to learn more about , there is another Moodle Skills Module called Academic Writing in LaTeX available.\nInput\n# Introduction\n\nText [@grolemund:2016, p. 361]\n\n# References\nturns into this output",
    "crumbs": [
      "FURTHER TOPICS",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>R Markdown</span>"
    ]
  },
  {
    "objectID": "12-downloads.html",
    "href": "12-downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "R Scripts\n\nBasics\nData Structures\nExploratory Analysis\nProgramming Concepts\nData Manipulation\nData Visualisation\nStatistical Association\nOLS\nLogit\nMarkdown\nSummary\n\n\n\nData Sets\n\nANES\nFearon\nScores\nTitanic\nWages\n\n\n\nExercise Solutions\n\nSolutions - Basics\nSolutions - Data Structures\nSolutions - Exploratory Analysis\nSolutions - Programming Concepts\nSolutions - Data Manipulation\nSolutions - Data Visualisation\nSolutions - Statistical Association\nSolutions - OLS\nSolutions - Logit\nSolutions - Markdown",
    "crumbs": [
      "DOCUMENTS",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Downloads</span>"
    ]
  }
]